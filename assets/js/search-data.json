{
  
    
        "post0": {
            "title": "농업 데이터 정재/가공/분석/모델링 교육",
            "content": ". Introduction . 이름 : 이강철 | . 회사 및 직책 : (주)좋은정보기술 주임 | . mail : rkdcjf8232@gmail.com | . 사용언어 : R | . R install . 교육 자료 : R for data science, 논문, ISLR 등등 | . 1. &#45936;&#51060;&#53552; &#51204;&#52376;&#47532;(tidyverse) . 데이터 전처리는 전체 분석 프로세스에서 가장 많은 시간이 소요되는 작업 | . 또한, 전처리 과정이 잘못 수행될 시에 분석 결과가 과대 또는 과소 해석으로 이루어질 수 있다. | . 실제 분석에서는 R의 벡터, 리스트 형태가 아닌 csv,xlsx 등의 파일을 데이터프레임 형태로 읽어들여 분석을 수행하기 때문에 데이터 프레임의 초점을 둘 것임 | . 왜 tidyverse냐? . tidyverse = dplyr + ggplot2 + $ dots$ | . 또한, 데이터 로드 및 전처리 수행시 R의 기존 내장 함수들보다 빠른 속도를 제공한다. | . pip(%&gt;%) 연선자 단축키 : ctrl + shift + m | 위에 단축키를 사용시 데이터셋을 말로 이어말하듯 전처리가 가능하다. | . | . 1. 데이터 로드 . 데이터 출처 : 농산물유통정보 KAMIS | . 해당 데이터는 2022년 1월 3일부터 2022년 6월 23일까지의 &quot;토마토와 딸기 도매가격&quot; | . 서울, 부산, 대구, 광주, 대전 지역의 도매가격이며 1kg당 단위 가격이다. | . #install.packages(&quot;nycflights13&quot;) # if not installed yet library(tidyverse) . setwd(&quot;C:/Users/git/Desktop/업무/20220624(업무인수인계)&quot;) . data &lt;- read_csv(&quot;total.csv&quot;) . Rows: 580 Columns: 4 -- Column specification Delimiter: &#34;,&#34; chr (2): 지역, 날짜 dbl (2): 토마토, 딸기 i Use `spec()` to retrieve the full column specification for this data. i Specify the column types or set `show_col_types = FALSE` to quiet this message. . head(data) . A tibble: 6 × 4 지역날짜토마토딸기 . &lt;chr&gt;&lt;chr&gt;&lt;dbl&gt;&lt;dbl&gt; . 서울 | 01월 03일 | 4950 | 12375 | . 부산 | 01월 03일 | 4000 | 10000 | . 대구 | 01월 03일 | 4400 | 11000 | . 광주 | 01월 03일 | 4720 | 11800 | . 대전 | 01월 03일 | 4630 | 11575 | . 서울 | 01월 04일 | 4950 | 12375 | . select &amp; fliter . select : 조건에 맞는 열 선택 | . filter : 조건에 맞는 행을 선택 | . 철수 : &quot;난 지역별, 날짜별 토마토의 도매가격을 알고 싶어&quot; . data %&gt;% select(지역, 날짜, 토마토) %&gt;% head() . A tibble: 6 × 3 지역날짜토마토 . &lt;chr&gt;&lt;chr&gt;&lt;dbl&gt; . 서울 | 01월 03일 | 4950 | . 부산 | 01월 03일 | 4000 | . 대구 | 01월 03일 | 4400 | . 광주 | 01월 03일 | 4720 | . 대전 | 01월 03일 | 4630 | . 서울 | 01월 04일 | 4950 | . 영희 : &quot;근데 서울의 인구수가 많으니까 서울 지역을 대표로 선택해서 보는 건 어때?&quot; . data %&gt;% select(지역, 날짜, 토마토) %&gt;% filter(지역==&quot;서울&quot;) %&gt;% head() . A tibble: 6 × 3 지역날짜토마토 . &lt;chr&gt;&lt;chr&gt;&lt;dbl&gt; . 서울 | 01월 03일 | 24750 | . 서울 | 01월 04일 | 24750 | . 서울 | 01월 05일 | 24750 | . 서울 | 01월 06일 | 24750 | . 서울 | 01월 07일 | 22850 | . 서울 | 01월 10일 | 20100 | . mutate . - 수식이나 문자표현식을 이용해 새로운 변수를 만드는 함수 . example 1 :(토마토 + 딸기) $ div$ 2$ quad to quad $ 평균가격 변수 생성 . data %&gt;% mutate(평균가격 =(토마토 + 딸기) / 2 ) %&gt;% head() . A tibble: 6 × 5 지역날짜토마토딸기평균가격 . &lt;chr&gt;&lt;chr&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt; . 서울 | 01월 03일 | 4950 | 12375 | 8662.5 | . 부산 | 01월 03일 | 4000 | 10000 | 7000.0 | . 대구 | 01월 03일 | 4400 | 11000 | 7700.0 | . 광주 | 01월 03일 | 4720 | 11800 | 8260.0 | . 대전 | 01월 03일 | 4630 | 11575 | 8102.5 | . 서울 | 01월 04일 | 4950 | 12375 | 8662.5 | . groupyby + summarize . 데이터의 그룹별 요약 통계량을 통해 그룹별 데이터의 형태를 파악할 수 있다. | . 현재 iris 데이터의 경우 총 3가지의 종으로 나누어져 있다. | . 1 각 그룹별 빈도수 세기 . data %&gt;% group_by(지역) %&gt;% count() . A grouped_df: 5 × 2 지역n . &lt;chr&gt;&lt;int&gt; . 광주 | 116 | . 대구 | 116 | . 대전 | 116 | . 부산 | 116 | . 서울 | 116 | . 2 각 지역별 토마토와 채소 평균가격 산출 . data %&gt;% group_by(지역) %&gt;% summarize(mean_tomato = mean(토마토), mean_strawberry = mean(딸기)) . A tibble: 5 × 3 지역mean_tomatomean_strawberry . &lt;chr&gt;&lt;dbl&gt;&lt;dbl&gt; . 광주 | 3285.000 | 8212.500 | . 대구 | 3370.690 | 8426.724 | . 대전 | 3386.810 | 8467.026 | . 부산 | 3189.310 | 7973.276 | . 서울 | 3240.603 | 8101.509 | . Join . 보통의 데이터는 한 가지의 데이터 프레임이 아닌 다수의 데이터 프레임을 결합하여 형성한다. | . 이러한 것을 데이터 프레임들을 &quot;join&quot; 한다고 하며 이 경우 각 데이터 프레임의 key 값을 이용한다. | . data1 &lt;- data %&gt;% select(1,2,3) names(data1)[1:2] &lt;- c(&quot;지역1&quot;,&quot;날짜1&quot;) data2 &lt;- data %&gt;% select(1,2,4) names(data2)[1:2] &lt;- c(&quot;지역2&quot;,&quot;날짜2&quot;) . head(data1) . A tibble: 6 × 3 지역1날짜1토마토 . &lt;chr&gt;&lt;chr&gt;&lt;dbl&gt; . 서울 | 01월 03일 | 4950 | . 부산 | 01월 03일 | 4000 | . 대구 | 01월 03일 | 4400 | . 광주 | 01월 03일 | 4720 | . 대전 | 01월 03일 | 4630 | . 서울 | 01월 04일 | 4950 | . head(data2) . A tibble: 6 × 3 지역2날짜2딸기 . &lt;chr&gt;&lt;chr&gt;&lt;dbl&gt; . 서울 | 01월 03일 | 12375 | . 부산 | 01월 03일 | 10000 | . 대구 | 01월 03일 | 11000 | . 광주 | 01월 03일 | 11800 | . 대전 | 01월 03일 | 11575 | . 서울 | 01월 04일 | 12375 | . data1 %&gt;% left_join(data2,by=c(&quot;지역1&quot;=&quot;지역2&quot;,&quot;날짜1&quot;=&quot;날짜2&quot;)) %&gt;% head() . A tibble: 6 × 4 지역1날짜1토마토딸기 . &lt;chr&gt;&lt;chr&gt;&lt;dbl&gt;&lt;dbl&gt; . 서울 | 01월 03일 | 4950 | 12375 | . 부산 | 01월 03일 | 4000 | 10000 | . 대구 | 01월 03일 | 4400 | 11000 | . 광주 | 01월 03일 | 4720 | 11800 | . 대전 | 01월 03일 | 4630 | 11575 | . 서울 | 01월 04일 | 4950 | 12375 | . . 2. &#45936;&#51060;&#53552; &#48516;&#49437; . &#51068;&#54364;&#48376; T &#44160;&#51221; . 개념 : 단일모집단에서 관심이 있는 연속형 벼수의 평균값을 특정기준값과 비교하고자 할 때 사용 | . mean(data$`토마토`) . 3294.48275862069 example :토마토 평균 도매가격이 16,400 인지 검정한다. . $$H_0 : mu = 3,200 quad H_1 : mu neq 3,200$$ . t.test(data$`토마토`,mu=3200,alternative= &quot;two.sided&quot;) . One Sample t-test data: data$토마토 t = 3.0602, df = 579, p-value = 0.002314 alternative hypothesis: true mean is not equal to 3200 95 percent confidence interval: 3233.844 3355.122 sample estimates: mean of x 3294.483 . 검정통계량의 근거한 p-value 값을 보았을 때 귀무가설을 기각한다. 즉, 토마토의 평균 도매가격은 3,200원이 아니다. | . &#45824;&#51025;&#54364;&#48376; T-&#44160;&#51221; . 단일모집단에 대해 두번의 처리를 가했을 때, 두 개의 처리에 따른 평균 차이를 비교하고자 할 때 사용 | . 표본 내의 개체들에 대해서 두 번의 측정을 실시 즉, 관측값들은 서로 독립적이지 않고 쌍으로 이루어져 있다. | . 상황 가정 : 코로나로 인해 토마토의 도매가격이 kg당 200원씩 상승했다. | . $$H_0 : mu_x = mu_y, quad H_1 = text{not} , , H_0$$ . $$H_0: mu_x - mu_y= 0 quad H_1 : text{not} , , H_0$$ . temp &lt;- data temp &lt;- temp %&gt;% mutate(after_t = 토마토 + 200) %&gt;% select(1,3,5) names(temp)[2:3] &lt;- c(&quot;토마토_before&quot;,&quot;토마토_after&quot;) temp %&gt;% head() . A tibble: 6 × 3 지역토마토_before토마토_after . &lt;chr&gt;&lt;dbl&gt;&lt;dbl&gt; . 서울 | 4950 | 5150 | . 부산 | 4000 | 4200 | . 대구 | 4400 | 4600 | . 광주 | 4720 | 4920 | . 대전 | 4630 | 4830 | . 서울 | 4950 | 5150 | . t.test(temp$`토마토_before`, temp$`토마토_after`,paird=T) . Welch Two Sample t-test data: temp$토마토_before and temp$토마토_after t = -4.5806, df = 1158, p-value = 5.139e-06 alternative hypothesis: true difference in means is not equal to 0 95 percent confidence interval: -285.667 -114.333 sample estimates: mean of x mean of y 3294.483 3494.483 . 검정통계량의 근거한 p-value 값을 보았을 때 귀무가설을 기각한다. 즉, 코로나 이후 토마토 도매가격의 평균 차이가 있다. | . &#46021;&#47549;&#54364;&#48376; T&#44160;&#51221; . 두 개의 독립된 모집단의 평균을 비교하고자 할 때 사용하는 검정 방법 | . 토마토와 딸기 kg당 도매가격의 유의미한 차이가 있는지 확인해보자 | . t.test(data$`토마토`,data$`딸기`) . Welch Two Sample t-test data: data$토마토 and data$딸기 t = -59.445, df = 759.66, p-value &lt; 2.2e-16 alternative hypothesis: true difference in means is not equal to 0 95 percent confidence interval: -5104.919 -4778.529 sample estimates: mean of x mean of y 3294.483 8236.207 . 검정통계량의 근거한 p-value 값을 보았을 때 귀무가설을 기각한다. 즉, 딸기와 토마토 평균 도매가격의 차이는 유의미하다. | . . &#51068;&#50896;&#48176;&#52824; &#48516;&#49328;&#48516;&#49437; . T-검정과 달리 두 개 이상의 다수 집단 간 평균을 비교하는 통계분석 방법 | . 반응값에 대한 하나의 범주형 변수(지역)의 영향을 알아보기 위해 사용한다. | . 지역별 토마토 도매가격의 평군차이가 있는지 확인해보자 | . result_1 &lt;- aov(토마토~지역,data) . summary(result_1) . Df Sum Sq Mean Sq F value Pr(&gt;F) 지역 4 3292778 823194 1.494 0.202 Residuals 575 316817167 550986 . F-통계량을 보았을 때 지역별 토마토 평균 도매가격의 차이는 존재하지 않는다. | . &#49345;&#44288;&#48516;&#49437; . 두 변수간 어느 정도의 상관성이 있는지 확인하는 분석 | . 상관분석에서 산출된 상관계수는 두 변수간의 인과관계를 설명해주진 않는다. | . 단지, 두 변수간 어느 정도의 연관성이 있는지 확인하는 척도이다. | . 현재 데이터의 날짜가 1월부터 6월까지이므로 점차 기온이 올라간다는 가정하에 임시 기온 데이터를 생성하고 강수량 데이터를 추가 | . te &lt;- 1:580 te &lt;- (te-1)/(580-1)*33 te &lt;- te + rnorm(580,mean=4,sd=2) data$온도 &lt;- te rain &lt;- rnorm(580,10,0.2) data$강수량 &lt;- rain . . cor(data %&gt;% select(3,5,6)) . A matrix: 3 × 3 of type dbl 토마토온도강수량 . 토마토 1.00000000 | -0.88597295 | 0.07388668 | . 온도-0.88597295 | 1.00000000 | -0.05656151 | . 강수량 0.07388668 | -0.05656151 | 1.00000000 | . cor(data %&gt;% select(4:6)) . A matrix: 3 × 3 of type dbl 딸기온도강수량 . 딸기 1.00000000 | -0.88597295 | 0.07388668 | . 온도-0.88597295 | 1.00000000 | -0.05656151 | . 강수량 0.07388668 | -0.05656151 | 1.00000000 | . &#54924;&#44480;&#48516;&#49437; . ‘회귀’라는 단어는 키가 큰 아버지의 아들 키가 세대를 거치면서 모집단의 평균키로 회귀한다는 발견에서 유래됨 | . 변수들간의 관계를 모형화하고 조사하는 통계적 기법 | . $$y= beta_0 + beta_1x_1+ dots beta_px_p$$ . $x$ : 독립변수, $y$ : 목표변수(예측변수) | . - 예제 : 온도와 강수량을 통해 토마토의 도매가격을 예측하는 모형 적합 . fit1 &lt;- lm(토마토~온도 + 강수량,data) summary(fit1) . Call: lm(formula = 토마토 ~ 온도 + 강수량, data = data) Residuals: Min 1Q Median 3Q Max -1055.91 -204.89 6.26 224.81 878.45 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 3831.335 676.771 5.661 2.37e-08 *** 온도 -66.744 1.457 -45.809 &lt; 2e-16 *** 강수량 83.245 67.400 1.235 0.217 Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 345 on 577 degrees of freedom Multiple R-squared: 0.7855, Adjusted R-squared: 0.7848 F-statistic: 1057 on 2 and 577 DF, p-value: &lt; 2.2e-16 . options(repr.plot.res=150,repr.plot.height=5,repr.plot.weight=10) par(mfrow= c(1,2)) plot(data$온도, data$토마토,main = &quot;토마토 ~ 온도&quot;, xlab=NULL,ylab=NULL) plot(data$온도, data$토마토,main = &quot;토마토 ~ 강수량&quot;, xlab=NULL,ylab=NULL) . . &#47784;&#54805;&#51652;&#45800; . 1 회귀모형의 유의성 파악 $ to$ F통계량에 근거한 p-value 값 살펴보기 . summary(fit1) . Call: lm(formula = 토마토 ~ 온도 + 강수량, data = data) Residuals: Min 1Q Median 3Q Max -1055.91 -204.89 6.26 224.81 878.45 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 3831.335 676.771 5.661 2.37e-08 *** 온도 -66.744 1.457 -45.809 &lt; 2e-16 *** 강수량 83.245 67.400 1.235 0.217 Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 345 on 577 degrees of freedom Multiple R-squared: 0.7855, Adjusted R-squared: 0.7848 F-statistic: 1057 on 2 and 577 DF, p-value: &lt; 2.2e-16 . 2 개별 회귀계수의 유의성파악 $ to$ t통계량에 근거한 p-value 값 살펴보기 . 3 결정계수 및 수정된 결정계수값 확인 . 4 잔차 진단 . options(repr.plot.res=200) par(mfrow=c(1,2)) plot(fit1,1) plot(fit1,2) .",
            "url": "https://gangcheol.github.io/data-mining/2022/06/24/%EA%B5%90%EC%9C%A1%EC%9E%90%EB%A3%8C.html",
            "relUrl": "/2022/06/24/%EA%B5%90%EC%9C%A1%EC%9E%90%EB%A3%8C.html",
            "date": " • Jun 24, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Ensemble",
            "content": "Bootstrap . - $n$개의 표본에서 샘플을 복원추출하는 방법 . example :$n$개의 표본에서 1번째 표본이 샘플로 채택되지 않을 확률 . $$ lim_{n to infty} left (1- frac{1}{n} right)^{n} = e^{-1} approx 0.367$$ . . Bagging . &#51060;&#47200; . 아이디어 : $B$개의 tree를 생성 후 앙상블한다. | . Step.1 : $n$개의 표본에서 $n$개의 샘플을 복원 추출 $ to L^{*(b)} quad b=1 dots B$ . Step.2 : tree 적합 $ to f^{(b)}(x)$ . Step.3: $B$개의 예측모형 결합 $ to hat f(x) = frac{1}{B} sum_{b=1}^{B} f^{(b)}(x)$ . - 분류모형의 경우 : B개 중 가장 많이 분류된 범주로 분류 . &#44592;&#45824;&#54952;&#44284; . tree의 경우 가지치기를 하지않으면 과적합문제가 발생 | . 배깅의 경우 $B$개의 tree를 생성 후 결과를 앙상블 하므로 예측모형의 분산감소 $ to$ 즉, 과적합을 막을 수 있음 | . 배깅방법은 tree 뿐만 아니라 여러 예측모형을 앙상블해서 사용할 수 있음 | . 단점 : 복원추출을 하여 모든 변수를 이용해 $f^{(b)}(x)$를 적합 시켰기 때문에 각각의 모형은 서로 독립이 보장되지 않는다. | . Out-of-Bag Error Estimation . 앞서 설명하였듯이 각 개체가 Boostrap sample에 속하지 않을확률은 $e^{-1} approx 0.367$이다. | . 즉, 각 개체는 $ frac {B}{3}$개의 트리에 포함되지 않는다. | . 이러한 데이터를 Out-of-bag 관측값(OOB)이라고 하며 이를 이용해 test error를 계산할수 있다. 포함되지 않은 tree에 개체를 집어넣은 후 평균을 내서 OOB test error 산출 | . | . . Random Forest . &#51060;&#47200; . boostrap sample + random sample of $m$ predictors | . 전체적인 방법은 배깅과 동일하나 각각의 $f^{(b)}(x)$에서 예측변수를 일부만 선택 회귀 문제 : $m = p/3$ | 분류 문제 : $m = sqrt p$ | . | . $f^{b}(x)$ 사이의 상관성을 줄이므로 분산의 감소를 일으킴 $ to$ 즉, 배깅보다 과적합을 더 잘 막음 | . 많은 수의 트리를 생성하기 때문에 예측력을 향상시킬 수 있지만, 트리의 강점인 설명력이 사라진다. | . 대신, 변수의 중유도를 측정할 수 있음 | . &#48320;&#49688;&#51473;&#50836;&#46020; . 1 정확도 감소 . $$d_i = r_i-e_b$$ . $$변수 중요도 = frac{ bar {d_i}}{s_{d_i}}$$ . $$s_{d_i}^2 = frac {1}{B-1} sum_{b=1}^{B}(d_{i}- bar {d_i})^2$$ . $e_b$ : $b$번째 예측모형에서의 OOB error 측정 | $r_i$ : 임의의 변수 $X_i$를 석었을 때 OOB error 측정 | . 2. 불순도 감소 . 트리 형성 과정에서 분리 규칙으로 사용된 변수들에 대해 $RSS$ 나 불순도가 얼마나 감소했는지를 측정 후 평균낸 값을 변수 중요도로 사용 | . . Boosting . &#51060;&#47200; . 약한 예측 모형을 결합하여 매우 정확한 예측 모형을 만듬 | . 배깅과의 차이점 $ to$ 배깅은 병렬, 부스팅은 순차적(직렬)으로 모형 결합 | . &#50508;&#44256;&#47532;&#51608; . 1 각 개체에 대해 초기 가중치 설정 : $w_i = frac 1n, , , i = 1,2,3 dots n$ . 2 가중치 $w_i$를 이용하여 boostrap sample을 추출하여 분류모형 생성 . 3 오차율 계산 . $$e_b= frac{ sum_{i=1}^{n} w_iI(y_i neq f_{b}(x_i))}{ sum_{i=1}^{n} w_i}$$ . $$ alpha_b = log frac {1-e_b}{e_b} $$ . 4 가중치 갱신 . $$ w_i leftarrow w_i times exp , left ( alpha_b I(y_i neq f_{b}(x_i)) right)$$ . 5 최종 모형 결정 : $f(x) = sign left( sum_{b=1}^{B} alpha_b f_b(x) right )$ . * sign(f(x)) &gt;0 : class A * sign(f(x)) &lt;0 : class B . &#51221;&#47532; . 배깅과 달리 예측력이 좋으나 과적합에 문제가 있음 $ to B$ 개수의 선택 중요 | . 약한 예측모형을 기본 모형으로 사용하기 때문에 하나의 부할을 갖는트리 (depth=2) 를 많이 사용함 | . . R&#49892;&#49845; . . library(randomForest) ##random Forest library(ipred) ##bagging library(gbm) ## Boosting library(xgboost) ## xgboost library(adabag) ##Adaboosting : boosting library(rpart) library(rpart.plot) library(tidyverse) library(data.table) . library(MASS) select &lt;- dplyr::select . tree &#47784;&#54805; . train &lt;- sample(1:nrow(Boston), nrow(Boston) / 2) Boston_train &lt;- Boston[train,-1] Boston_test &lt;- Boston[-train,-1] cart_boston&lt;- rpart(medv ~ ., data=Boston_train) cart_boston . n= 253 node), split, n, deviance, yval * denotes terminal node 1) root 253 21715.9700 22.67668 2) rm&lt; 6.676 199 6547.3640 19.29095 4) lstat&gt;=14.91 77 1535.7650 14.54026 8) tax&gt;=551.5 43 466.4065 11.92791 * 9) tax&lt; 551.5 34 404.7838 17.84412 * 5) lstat&lt; 14.91 122 2176.9560 22.28934 10) lstat&gt;=9.89 63 321.4965 20.05397 * 11) lstat&lt; 9.89 59 1204.5070 24.67627 22) dis&gt;=2.96955 51 307.4016 23.87255 * 23) dis&lt; 2.96955 8 654.1400 29.80000 * 3) rm&gt;=6.676 54 4480.9340 35.15370 6) rm&lt; 7.435 39 2005.1790 31.27179 12) lstat&gt;=8.76 9 357.9422 23.75556 * 13) lstat&lt; 8.76 30 986.2587 33.52667 26) lstat&gt;=4.6 22 254.1436 31.47273 * 27) lstat&lt; 4.6 8 384.0750 39.17500 * 7) rm&gt;=7.435 15 360.0373 45.24667 * . rpart.plot(cart_boston, main = &quot;Regression using CART&quot;) . summary(cart_boston) . Call: rpart(formula = medv ~ ., data = Boston_train) n= 253 CP nsplit rel error xerror xstd 1 0.49215730 0 1.0000000 1.0144534 0.11691897 2 0.13053260 1 0.5078427 0.6066988 0.07865966 3 0.09742681 2 0.3773101 0.4638606 0.06743349 4 0.03060304 3 0.2798833 0.3369510 0.05919318 5 0.03043742 4 0.2492802 0.3172014 0.05609988 6 0.02997576 5 0.2188428 0.3100736 0.05509320 7 0.01602691 6 0.1888671 0.2625877 0.04884343 8 0.01118832 7 0.1728401 0.2536600 0.04383761 9 0.01000000 8 0.1616518 0.2513963 0.04522195 Variable importance rm lstat indus dis nox ptratio tax age zn rad 34 25 10 6 6 5 5 5 3 2 Node number 1: 253 observations, complexity param=0.4921573 mean=22.67668, MSE=85.83388 left son=2 (199 obs) right son=3 (54 obs) Primary splits: rm &lt; 6.676 to the left, improve=0.4921573, (0 missing) lstat &lt; 9.95 to the right, improve=0.4721697, (0 missing) nox &lt; 0.6695 to the right, improve=0.2517992, (0 missing) indus &lt; 7.015 to the right, improve=0.2313864, (0 missing) ptratio &lt; 19.65 to the right, improve=0.2210499, (0 missing) Surrogate splits: lstat &lt; 5.495 to the right, agree=0.874, adj=0.407, (0 split) indus &lt; 3.985 to the right, agree=0.818, adj=0.148, (0 split) ptratio &lt; 13.85 to the right, agree=0.810, adj=0.111, (0 split) zn &lt; 87.5 to the left, agree=0.806, adj=0.093, (0 split) Node number 2: 199 observations, complexity param=0.1305326 mean=19.29095, MSE=32.90133 left son=4 (77 obs) right son=5 (122 obs) Primary splits: lstat &lt; 14.91 to the right, improve=0.4329441, (0 missing) nox &lt; 0.6695 to the right, improve=0.3370918, (0 missing) dis &lt; 2.2466 to the left, improve=0.2615125, (0 missing) indus &lt; 16.57 to the right, improve=0.2574271, (0 missing) ptratio &lt; 19.9 to the right, improve=0.2573377, (0 missing) Surrogate splits: age &lt; 88.55 to the right, agree=0.814, adj=0.519, (0 split) dis &lt; 2.09525 to the left, agree=0.799, adj=0.481, (0 split) nox &lt; 0.619 to the right, agree=0.784, adj=0.442, (0 split) indus &lt; 16.57 to the right, agree=0.774, adj=0.416, (0 split) tax &lt; 434.5 to the right, agree=0.749, adj=0.351, (0 split) Node number 3: 54 observations, complexity param=0.09742681 mean=35.1537, MSE=82.98026 left son=6 (39 obs) right son=7 (15 obs) Primary splits: rm &lt; 7.435 to the left, improve=0.47216000, (0 missing) lstat &lt; 4.68 to the right, improve=0.39060180, (0 missing) ptratio &lt; 14.75 to the right, improve=0.16885410, (0 missing) black &lt; 390.815 to the right, improve=0.12146710, (0 missing) dis &lt; 2.47725 to the right, improve=0.09537131, (0 missing) Surrogate splits: lstat &lt; 4.355 to the right, agree=0.833, adj=0.400, (0 split) indus &lt; 18.84 to the left, agree=0.741, adj=0.067, (0 split) ptratio &lt; 14.75 to the right, agree=0.741, adj=0.067, (0 split) Node number 4: 77 observations, complexity param=0.03060304 mean=14.54026, MSE=19.945 left son=8 (43 obs) right son=9 (34 obs) Primary splits: tax &lt; 551.5 to the right, improve=0.4327321, (0 missing) ptratio &lt; 19.65 to the right, improve=0.4162932, (0 missing) rad &lt; 16 to the right, improve=0.3745379, (0 missing) dis &lt; 2.0037 to the left, improve=0.3513200, (0 missing) indus &lt; 16.01 to the right, improve=0.3320846, (0 missing) Surrogate splits: rad &lt; 16 to the right, agree=0.987, adj=0.971, (0 split) ptratio &lt; 19.65 to the right, agree=0.883, adj=0.735, (0 split) indus &lt; 16.01 to the right, agree=0.818, adj=0.588, (0 split) nox &lt; 0.5825 to the right, agree=0.818, adj=0.588, (0 split) dis &lt; 2.45285 to the left, agree=0.779, adj=0.500, (0 split) Node number 5: 122 observations, complexity param=0.02997576 mean=22.28934, MSE=17.8439 left son=10 (63 obs) right son=11 (59 obs) Primary splits: lstat &lt; 9.89 to the right, improve=0.29901970, (0 missing) rm &lt; 6.1405 to the left, improve=0.14127680, (0 missing) indus &lt; 3.085 to the right, improve=0.08219182, (0 missing) nox &lt; 0.5125 to the right, improve=0.05999634, (0 missing) tax &lt; 278 to the right, improve=0.05927320, (0 missing) Surrogate splits: rm &lt; 6.029 to the left, agree=0.787, adj=0.559, (0 split) nox &lt; 0.496 to the right, agree=0.754, adj=0.492, (0 split) dis &lt; 4.0711 to the left, agree=0.713, adj=0.407, (0 split) indus &lt; 7.76 to the right, agree=0.705, adj=0.390, (0 split) age &lt; 32.8 to the right, agree=0.689, adj=0.356, (0 split) Node number 6: 39 observations, complexity param=0.03043742 mean=31.27179, MSE=51.41485 left son=12 (9 obs) right son=13 (30 obs) Primary splits: lstat &lt; 8.76 to the right, improve=0.32963550, (0 missing) rm &lt; 6.9715 to the left, improve=0.11851220, (0 missing) age &lt; 90.8 to the left, improve=0.10879450, (0 missing) dis &lt; 2.0718 to the right, improve=0.10879450, (0 missing) black &lt; 394.75 to the right, improve=0.05804145, (0 missing) Surrogate splits: nox &lt; 0.659 to the right, agree=0.821, adj=0.222, (0 split) rm &lt; 6.7275 to the left, agree=0.795, adj=0.111, (0 split) dis &lt; 2.8885 to the left, agree=0.795, adj=0.111, (0 split) tax &lt; 417.5 to the right, agree=0.795, adj=0.111, (0 split) ptratio &lt; 19.7 to the right, agree=0.795, adj=0.111, (0 split) Node number 7: 15 observations mean=45.24667, MSE=24.00249 Node number 8: 43 observations mean=11.92791, MSE=10.84666 Node number 9: 34 observations mean=17.84412, MSE=11.90541 Node number 10: 63 observations mean=20.05397, MSE=5.103119 Node number 11: 59 observations, complexity param=0.01118832 mean=24.67627, MSE=20.41537 left son=22 (51 obs) right son=23 (8 obs) Primary splits: dis &lt; 2.96955 to the right, improve=0.20171340, (0 missing) age &lt; 82.65 to the left, improve=0.13923170, (0 missing) indus &lt; 13.375 to the left, improve=0.12326910, (0 missing) rad &lt; 6.5 to the left, improve=0.09182361, (0 missing) black &lt; 387.13 to the right, improve=0.06285260, (0 missing) Surrogate splits: nox &lt; 0.5585 to the left, agree=0.949, adj=0.625, (0 split) age &lt; 87.65 to the left, agree=0.949, adj=0.625, (0 split) indus &lt; 16.57 to the left, agree=0.932, adj=0.500, (0 split) black &lt; 355.065 to the right, agree=0.915, adj=0.375, (0 split) rad &lt; 16 to the left, agree=0.898, adj=0.250, (0 split) Node number 12: 9 observations mean=23.75556, MSE=39.77136 Node number 13: 30 observations, complexity param=0.01602691 mean=33.52667, MSE=32.87529 left son=26 (22 obs) right son=27 (8 obs) Primary splits: lstat &lt; 4.6 to the right, improve=0.3528892, (0 missing) age &lt; 78.45 to the left, improve=0.3147086, (0 missing) dis &lt; 3.0655 to the right, improve=0.2890375, (0 missing) tax &lt; 384 to the left, improve=0.2544259, (0 missing) nox &lt; 0.521 to the left, improve=0.2211543, (0 missing) Surrogate splits: dis &lt; 1.88595 to the right, agree=0.833, adj=0.375, (0 split) tax &lt; 400 to the left, agree=0.833, adj=0.375, (0 split) ptratio &lt; 18.55 to the left, agree=0.833, adj=0.375, (0 split) zn &lt; 75 to the left, agree=0.800, adj=0.250, (0 split) indus &lt; 16.01 to the left, agree=0.800, adj=0.250, (0 split) Node number 22: 51 observations mean=23.87255, MSE=6.027482 Node number 23: 8 observations mean=29.8, MSE=81.7675 Node number 26: 22 observations mean=31.47273, MSE=11.55198 Node number 27: 8 observations mean=39.175, MSE=48.00938 . 변수중요도 | . cart_boston$variable.importance . &lt;dl class=dl-inline&gt;rm13240.926671874lstat9695.13826464061indus3755.62051955766dis2406.10492751623nox2261.28381425972ptratio2021.18225080163tax1862.49738906225age1856.08983679273zn1076.60949213269rad705.769843004862black91.111954137587&lt;/dl&gt; cart_boston$cptable ##complexity parameter . A matrix: 9 × 5 of type dbl CPnsplitrel errorxerrorxstd . 10.49215730 | 0 | 1.0000000 | 1.0144534 | 0.11691897 | . 20.13053260 | 1 | 0.5078427 | 0.6066988 | 0.07865966 | . 30.09742681 | 2 | 0.3773101 | 0.4638606 | 0.06743349 | . 40.03060304 | 3 | 0.2798833 | 0.3369510 | 0.05919318 | . 50.03043742 | 4 | 0.2492802 | 0.3172014 | 0.05609988 | . 60.02997576 | 5 | 0.2188428 | 0.3100736 | 0.05509320 | . 70.01602691 | 6 | 0.1888671 | 0.2625877 | 0.04884343 | . 80.01118832 | 7 | 0.1728401 | 0.2536600 | 0.04383761 | . 90.01000000 | 8 | 0.1616518 | 0.2513963 | 0.04522195 | . 가지치기 판단기준 가장 작은 xerror+ xstd 값이 들어있는 xerror범위의 cp값을 채택 | . plotcp(cart_boston) . 가지치기 | . prune_cart_boston &lt;- rpart(medv ~ ., data=Boston_train, control = rpart.control(cp = 0.016)) . rpart.plot(prune_cart_boston, main = &quot;Regression using CART&quot;) . 예측 | . yhat &lt;- predict(prune_cart_boston, newdata = Boston_test) . 가지치기 후 MSE | . mean((yhat-Boston_test$medv)^2) ###MSE . 26.7515016503775 가지치기전 MSE | . mean((predict(cart_boston, newdata=Boston_test)-Boston_test$medv)^2) . 25.9656807238935 bagging . fit.bagg&lt;- ipredbagg(Boston_train$medv, Boston_train[,-13], nbagg=1000, ## 트리 형성 개수 coob=T) ## oob error estimator를 측정할 것이냐 말거냐 . fit.bagg . Bagging regression trees with 1000 bootstrap replications Out-of-bag estimate of root mean squared error: 4.2268 . 예측 | . pred&lt;-predict(fit.bagg, newdata = Boston_test) mean((pred-Boston_test$medv)^2) ###MSE . 17.5008988860572 Random Forest . rf.boston &lt;- randomForest(medv ~ ., data = Boston_train, ntree=200, ## 트리개 수 mtry = 4, ## 설명변수 개수 importance = TRUE ## 변수 중요도 출력 여부 ) . rf.boston . Call: randomForest(formula = medv ~ ., data = Boston_train, ntree = 200, mtry = 4, importance = TRUE) Type of random forest: regression Number of trees: 200 No. of variables tried at each split: 4 Mean of squared residuals: 14.83848 % Var explained: 82.71 . % Var explained: 82.71 $ to$ 설명력 정도 | . yhat.bag &lt;- predict(rf.boston, newdata = Boston_test) . plot(yhat.bag, Boston_test$medv, pch=16) abline(0, 1, col=&#39;steelblue&#39;) . test MSE 계산 | . mean((yhat.bag - Boston_test$medv)^2) . 11.2972925930259 plot(rf.boston$mse, type=&#39;l&#39;, lwd=2,main=&quot;tree 개수에 따른 oob sample의 MSE&quot;) . plot(rf.boston$rsq, type=&#39;l&#39;, lwd=2, main = &quot;tree 개수에 따른 oob sample의 R square&quot;) . IncNodePurity : 해당 노드에서 순수도 증가량 $ to$ 회귀에서는 RSS | . %incMSE : oob sample에 대한 MSE 증가량 | . 두 수치 전부 클수록 중요한 변수 | . rf.boston$importance . A matrix: 12 × 2 of type dbl %IncMSEIncNodePurity . zn 0.9674468 | 155.3546 | . indus 4.3008534 | 932.0667 | . chas 2.9736723 | 372.1183 | . nox 9.3052974 | 1879.1228 | . rm37.3644011 | 6638.7398 | . age 1.7070941 | 563.9708 | . dis 8.7043872 | 1453.4294 | . rad 1.3373895 | 191.3632 | . tax 3.8467383 | 704.0282 | . ptratio 5.8036452 | 1301.5738 | . black 1.0808407 | 549.4985 | . lstat54.1087324 | 6478.5206 | . rf.boston$importanceSD . &lt;dl class=dl-inline&gt;zn0.694329862910252indus0.986119419805129chas0.694885969673947nox1.0697941298143rm2.07293992540765age0.468706790498745dis0.885127811072486rad0.388507429703034tax0.667640467971253ptratio0.826551063811327black0.342822269254035lstat3.4464005481774&lt;/dl&gt; varImpPlot(rf.boston, pch=16) . oob.times : 200개의 트리 각각에서 선택된 oob sample | . rf.boston$oob.times . &lt;ol class=list-inline&gt;75 | 62 | 71 | 70 | 87 | 77 | 78 | 78 | 80 | 82 | 73 | 82 | 71 | 64 | 71 | 83 | 77 | 65 | 65 | 73 | 80 | 66 | 81 | 65 | 59 | 83 | 76 | 77 | 75 | 85 | 57 | 69 | 67 | 81 | 65 | 68 | 73 | 83 | 80 | 61 | 72 | 64 | 80 | 64 | 63 | 75 | 74 | 68 | 69 | 74 | 83 | 72 | 75 | 71 | 75 | 75 | 63 | 77 | 91 | 84 | 75 | 61 | 66 | 74 | 72 | 73 | 75 | 64 | 69 | 79 | 78 | 75 | 73 | 76 | 72 | 68 | 69 | 80 | 66 | 69 | 79 | 80 | 71 | 81 | 85 | 80 | 76 | 70 | 73 | 80 | 68 | 76 | 81 | 61 | 68 | 66 | 59 | 80 | 71 | 77 | 75 | 84 | 78 | 59 | 77 | 77 | 62 | 82 | 83 | 72 | 66 | 74 | 72 | 72 | 63 | 74 | 71 | 69 | 73 | 88 | 73 | 77 | 92 | 75 | 71 | 81 | 66 | 80 | 79 | 82 | 78 | 71 | 76 | 80 | 67 | 69 | 77 | 86 | 72 | 73 | 62 | 78 | 60 | 71 | 63 | 73 | 66 | 65 | 67 | 77 | 65 | 78 | 66 | 73 | 63 | 74 | 61 | 68 | 69 | 58 | 75 | 68 | 69 | 79 | 66 | 73 | 78 | 75 | 82 | 74 | 68 | 74 | 66 | 81 | 74 | 63 | 74 | 63 | 70 | 73 | 67 | 63 | 75 | 64 | 74 | 68 | 82 | 68 | 77 | 74 | 67 | 72 | 79 | 67 | 80 | 75 | 68 | 77 | 66 | 75 | 79 | 69 | 86 | 68 | 79 | 80 | 80 | 72 | 90 | 76 | 71 | 72 | 68 | 66 | 62 | 79 | 73 | 82 | 87 | 71 | 86 | 79 | 69 | 76 | 80 | 75 | 66 | 74 | 68 | 74 | 74 | 63 | 70 | 75 | 70 | 65 | 79 | 71 | 75 | 72 | 67 | 74 | 85 | 65 | 75 | 79 | 66 | 81 | 64 | 70 | 78 | 71 | 69 | &lt;/ol&gt; par(mfrow=c(1,2)) barplot(sort(importance(rf.boston)[,&quot;%IncMSE&quot;], decreasing = T), horiz = T,las=1, col=&#39;skyblue&#39;, xlab = &quot;%IncMSE&quot;) barplot(sort(importance(rf.boston)[,&quot;IncNodePurity&quot;], decreasing = T), horiz = T,las=1, col=&#39;skyblue&#39;, xlab = &quot;IncNodePurity&quot;) . 변수 개수에 따른 mse 산출 | . bag.boston &lt;- randomForest(medv ~ ., data = Boston_train, mtry = 13) . Warning message in randomForest.default(m, y, ...): &#34;invalid mtry: reset to within valid range&#34; . rf.boston &lt;- randomForest(medv ~ ., data = Boston_train, mtry = 4, xtest=Boston_test[,-13], ytest=Boston_test$medv) . mtry_rf &lt;- function(m){ return(randomForest(medv ~ ., data = Boston_train, mtry = m)$mse) } . tmp_dt &lt;- data.table( num_tree = 1:500, rf_1 = mtry_rf(1), rf_4 = rf.boston$mse, rf_8 = mtry_rf(8), rf_13 = bag.boston$mse, rf_test = rf.boston$test$mse ## 변수가 4애일 때 test mse ) . melt.tmp &lt;- melt(tmp_dt, id=1) . ggplot(melt.tmp, aes(num_tree, value, col=variable)) + geom_line(lwd=1) + labs(y=&#39;MSE&#39;, col=&quot;&quot;) + theme_bw() . . Boosting . boosting_Boston&lt;- gbm(medv~., data=Boston_train, distribution=&quot;gaussian&quot;, ## 회귀문제일 경우 interaction.depth = 1, ## tree의 depth n.trees=500) boosting_Boston . gbm(formula = medv ~ ., distribution = &#34;gaussian&#34;, data = Boston_train, n.trees = 500, interaction.depth = 1) A gradient boosted model with gaussian loss function. 500 iterations were performed. There were 12 predictors of which 11 had non-zero influence. . 12 predictors of which 11 had non-zero influence $ to$ 12개의 변수 모두 중요한 변수이다. | . 변수 중요도 출력 rel.inf : 상대적 변수 중요도 | . | . summary(boosting_Boston) . A data.frame: 12 × 2 varrel.inf . &lt;chr&gt;&lt;dbl&gt; . lstatlstat | 37.2755807 | . rmrm | 33.6693770 | . disdis | 10.0696331 | . noxnox | 6.0473471 | . ageage | 2.9565208 | . ptratioptratio | 2.7893943 | . blackblack | 2.6728669 | . chaschas | 1.8357763 | . indusindus | 1.1517103 | . taxtax | 1.0362397 | . radrad | 0.4955539 | . znzn | 0.0000000 | . rm 변수와 y의 관계 | . plot(boosting_Boston, i = &quot;rm&quot;) . plot(boosting_Boston, i = &quot;lstat&quot;) . 예측 | . yhat.boost &lt;- predict(boosting_Boston, newdata = Boston_test, n.trees = 500) . plot(Boston_test$medv, yhat.boost, pch=16) abline(a=0, b=1, col=&#39;blue&#39;) . mean((yhat.boost - Boston_test$medv)^2) ##Test MSE . 13.7155972065924 plot(boosting_Boston$train.error, type=&#39;l&#39;,main = &quot;tree 개수에 따른 train error&quot;) . plot(boosting_Boston$oobag.improve,type=&#39;l&#39;, main= &quot;tree 개수에 따른 oob error&quot;) . boosting with cv . boosting_Boston&lt;- gbm(medv~.,data=Boston_train, distribution=&quot;gaussian&quot;, interaction.depth = 4, cv.folds = 10, ## 10-fold n.trees=500) . 파란색은 cv일 때 optimal 트리 | . gbm.perf(boosting_Boston, plot.it = TRUE, oobag.curve = F, overlay = TRUE, method=&#39;cv&#39;) . 189 oob sample일 때 최적 트리 개수 | . gbm.perf(boosting_Boston, plot.it = TRUE, oobag.curve = F, overlay = TRUE, method=&#39;OOB&#39;) . OOB generally underestimates the optimal number of iterations although predictive performance is reasonably competitive. Using cv_folds&gt;1 when calling gbm usually results in improved predictive performance. . 25 깊이에 따른 변화관측 | . Boost_ID &lt;- function(d){ tmp_boost_model &lt;- gbm(medv~.,data=Boston_train, distribution=&quot;gaussian&quot;, interaction.depth = d, n.trees=100) return(tmp_boost_model$oobag.improve) } . tmp_dt &lt;- data.table( num_tree = 1:100, Boost_1 = Boost_ID(1), Boost_2 = Boost_ID(2), Boost_4 = Boost_ID(4), Boost_12 = Boost_ID(12) ) melt.tmp &lt;- melt(tmp_dt, id=1) ggplot(melt.tmp, aes(num_tree, value, col=variable)) + geom_line(lwd=2) + labs(y=&#39;oobag.improve&#39;, col=&quot;depth&quot;)+ theme_bw() . 결과를 보니 depth의 개수에 따라 변화가 미비하다 즉, depth를 1개만 해도 되겠당 | . .",
            "url": "https://gangcheol.github.io/data-mining/2022/06/24/Ensemble.html",
            "relUrl": "/2022/06/24/Ensemble.html",
            "date": " • Jun 24, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "표준화",
            "content": "각 개체들이 평균을 기준으로 얼마나 떨어져 있는지를 나타내는 값으로 변환하는 과정 | . Z-Score 표준화는 각 요소의 값에서 평균을 뺀 후 표준편차로 나누어 수행 | . 변환 후 데이터의 평균은 0, 표준편차는 1의 값을 갖게 된다. | . R&#49892;&#49845; . scale center : TRUE이면 데이터에서 해당벡터의 평균을 뺌 | scale : center = T, scale = T 이면 데이터를 해당 벡터의 표준편차로 나눔 | center = F, scale = T 이면 데이터를 해당 벡터의 제곱평균제곱근으로 나눔 | scale = F 이면 데이터를 어떤 값으로도 나누지 않음 | . | . | . mtcars 데이터의 mpg, hp 변수로만 이루어진 데이터프레임을 생성하고 각 변수를 표준화한 새로운 변수를 추가해보자. | . library(tidyverse) . test &lt;- mtcars %&gt;% select(mpg,hp) . test %&gt;% transmute_at(vars(mpg,hp),scale) %&gt;% head() ##scale(test$mpg, center=T,scale=T) 안되면 걍이런식으로 노가다 붙이자 . A data.frame: 6 × 2 mpghp . &lt;dbl[,1]&gt;&lt;dbl[,1]&gt; . Mazda RX4 0.1508848 | -0.5350928 | . Mazda RX4 Wag 0.1508848 | -0.5350928 | . Datsun 710 0.4495434 | -0.7830405 | . Hornet 4 Drive 0.2172534 | -0.5350928 | . Hornet Sportabout-0.2307345 | 0.4129422 | . Valiant-0.3302874 | -0.6080186 | . &#51221;&#44508;&#54868; . 정규화란 데이터의 범위를 0과 1사이로 변환하여 데이터의 분포를 조정하는 방법 | . 특정 개체가 가지는 위치를 파악하고 비교할 때 유용하게 사용할 수 있다. | . R&#49892;&#49845; . 사용자 정의함수 생성 | . normal &lt;- function(x) { return ((x- min(x)) / (max(x)-min(x))) } . iris2 &lt;- iris %&gt;% transmute_at(vars(-Species),normal) . head(iris); head(iris2) . A data.frame: 6 × 5 Sepal.LengthSepal.WidthPetal.LengthPetal.WidthSpecies . &lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;fct&gt; . 15.1 | 3.5 | 1.4 | 0.2 | setosa | . 24.9 | 3.0 | 1.4 | 0.2 | setosa | . 34.7 | 3.2 | 1.3 | 0.2 | setosa | . 44.6 | 3.1 | 1.5 | 0.2 | setosa | . 55.0 | 3.6 | 1.4 | 0.2 | setosa | . 65.4 | 3.9 | 1.7 | 0.4 | setosa | . A data.frame: 6 × 4 Sepal.LengthSepal.WidthPetal.LengthPetal.Width . &lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt; . 10.22222222 | 0.6250000 | 0.06779661 | 0.04166667 | . 20.16666667 | 0.4166667 | 0.06779661 | 0.04166667 | . 30.11111111 | 0.5000000 | 0.05084746 | 0.04166667 | . 40.08333333 | 0.4583333 | 0.08474576 | 0.04166667 | . 50.19444444 | 0.6666667 | 0.06779661 | 0.04166667 | . 60.30555556 | 0.7916667 | 0.11864407 | 0.12500000 | .",
            "url": "https://gangcheol.github.io/data-mining/python/2022/06/23/%ED%91%9C%EC%A4%80%ED%99%94%EC%99%80-%EC%A0%95%EA%B7%9C%ED%99%94(1205).html",
            "relUrl": "/python/2022/06/23/%ED%91%9C%EC%A4%80%ED%99%94%EC%99%80-%EC%A0%95%EA%B7%9C%ED%99%94(1205).html",
            "date": " • Jun 23, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "SAMPLING",
            "content": "1. &#45800;&#49692; &#51076;&#51032; &#52628;&#52636; . n &lt;- nrow(iris) . idx &lt;- sample(1:n,n*0.7,replace=F) . train &lt;- iris[idx,] test &lt;- iris[-idx] . 2. &#52789;&#54868; &#51076;&#51032; &#52628;&#52636; . method 1. srswor : 비복원 단순 임의 추출 2. srswr : 복원 단순 임의 추출 3. possion : 포아송 추출 4. systematic : 계통 추출 | . library(sampling) . samples &lt;- strata(iris,c(&quot;Species&quot;),size=c(20,15,15),method=&quot;srswor&quot;) . iris_sample &lt;- getdata(iris,samples) . 3. &#44228;&#53685; &#52628;&#52636; . formula : ~ 우축에 나열한 이름에 따라 데이터가 그룹으로 묶임 | frac = 0.1 : 추출할 샘플 비율 기본값은 10% | replace : 복원 추출 여부 | data = parent.frame() : 추출할 데이터 프레임 | systematic = F : 계통 추출(Systematic Sampling)을 사용할지 여부 | . library(doBy) . head(sampleBy(~ Species,frac = 0.3,data = iris, systematic =T)) . A data.frame: 6 × 5 Sepal.LengthSepal.WidthPetal.LengthPetal.WidthSpecies . &lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;fct&gt; . setosa.15.1 | 3.5 | 1.4 | 0.2 | setosa | . setosa.44.6 | 3.1 | 1.5 | 0.2 | setosa | . setosa.74.6 | 3.4 | 1.4 | 0.3 | setosa | . setosa.115.4 | 3.7 | 1.5 | 0.2 | setosa | . setosa.144.3 | 3.0 | 1.1 | 0.1 | setosa | . setosa.175.4 | 3.9 | 1.3 | 0.4 | setosa | .",
            "url": "https://gangcheol.github.io/data-mining/2022/06/23/%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%83%98%ED%94%8C%EB%A7%81.html",
            "relUrl": "/2022/06/23/%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%83%98%ED%94%8C%EB%A7%81.html",
            "date": " • Jun 23, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "1",
            "content": "library(tidyverse) library(arules) library(rpart) library(rpart.plot) library(ipred) library(gbm) library(randomForest) library(caret) setwd(&quot;C:/Users/lee/Desktop/데이터마이닝특강/Final Project&quot;) . Warning message: &#34;패키지 &#39;caret&#39;는 R 버전 4.1.3에서 작성되었습니다&#34; . Error: package or namespace load failed for &#39;caret&#39; in loadNamespace(j &lt;- i[[1L]], c(lib.loc, .libPaths()), versionCheck = vI[[j]]): &#39;hardhat&#39;이라고 불리는 패키지가 없습니다 Traceback: 1. library(caret) 2. tryCatch({ . attr(package, &#34;LibPath&#34;) &lt;- which.lib.loc . ns &lt;- loadNamespace(package, lib.loc) . env &lt;- attachNamespace(ns, pos = pos, deps, exclude, include.only) . }, error = function(e) { . P &lt;- if (!is.null(cc &lt;- conditionCall(e))) . paste(&#34; in&#34;, deparse(cc)[1L]) . else &#34;&#34; . msg &lt;- gettextf(&#34;package or namespace load failed for %s%s: n %s&#34;, . sQuote(package), P, conditionMessage(e)) . if (logical.return &amp;&amp; !quietly) . message(paste(&#34;Error:&#34;, msg), domain = NA) . else stop(msg, call. = FALSE, domain = NA) . }) 3. tryCatchList(expr, classes, parentenv, handlers) 4. tryCatchOne(expr, names, parentenv, handlers[[1L]]) 5. value[[3L]](cond) 6. stop(msg, call. = FALSE, domain = NA) . (a) . data(Income) inspect(Income[1]) . items transactionID [1] {income=$40,000+, sex=male, marital status=married, age=35+, education=college graduate, occupation=homemaker, years in bay area=10+, dual incomes=no, number in household=2+, number of children=1+, householder status=own, type of home=house, ethnic classification=white, language in home=english} 2 . (b) . glimpse(Income) v &lt;- tibble(unique(Income@itemInfo$variables)) v &lt;- v %&gt;% mutate(col_index = 1:nrow(v)) %&gt;% select(2,1) names(v)[2] &lt;- &quot;variable&quot; write_excel_csv(v,&quot;1-b.csv&quot;) . Formal class &#39;transactions&#39; [package &#34;arules&#34;] with 3 slots ..@ data :Formal class &#39;ngCMatrix&#39; [package &#34;Matrix&#34;] with 5 slots ..@ itemInfo :&#39;data.frame&#39;: 50 obs. of 3 variables: .. ..$ labels : chr [1:50] &#34;income=$0-$40,000&#34; &#34;income=$40,000+&#34; &#34;sex=male&#34; &#34;sex=female&#34; ... .. ..$ variables: Factor w/ 14 levels &#34;age&#34;,&#34;dual incomes&#34;,..: 6 6 12 12 8 8 8 8 8 1 ... .. ..$ levels : Factor w/ 48 levels &#34;0&#34;,&#34;$0-$40,000&#34;,..: 2 10 28 22 29 16 19 47 42 6 ... ..@ itemsetInfo:&#39;data.frame&#39;: 6876 obs. of 1 variable: .. ..$ transactionID: chr [1:6876] &#34;2&#34; &#34;3&#34; &#34;4&#34; &#34;5&#34; ... . (c) . Income_4 &lt;- Income[Income %in% &quot;income=$40,000+&quot;] itemFrequencyPlot(Income_4,support=0.2,topN=10,main=&quot;itemFrequecyPlot support &gt; 0.2&quot;) . (d) . rules &lt;- apriori(Income_4,parameter = list(support=0.1, confidence=0.8)) rules_1 &lt;- subset(rules, subset = rhs %in% &quot;income=$40,000+&quot; &amp; lift ==1) rules_1 &lt;- subset(rules, subset = !(rhs %in% &quot;dual incomes=not married&quot;)) . Apriori Parameter specification: confidence minval smax arem aval originalSupport maxtime support minlen 0.8 0.1 1 none FALSE TRUE 5 0.1 1 maxlen target ext 10 rules TRUE Algorithmic control: filter tree heap memopt load sort verbose 0.1 TRUE TRUE FALSE TRUE 2 TRUE Absolute minimum support count: 259 set item appearances ...[0 item(s)] done [0.00s]. set transactions ...[49 item(s), 2596 transaction(s)] done [0.00s]. sorting and recoding items ... [26 item(s)] done [0.00s]. creating transaction tree ... done [0.00s]. checking subsets of size 1 2 3 4 5 6 7 8 9 10 . Warning message in apriori(Income_4, parameter = list(support = 0.1, confidence = 0.8)): &#34;Mining stopped (maxlen reached). Only patterns up to a length of 10 returned!&#34; . done [0.02s]. writing ... [29323 rule(s)] done [0.00s]. creating S4 object ... done [0.01s]. . top5 &lt;- inspect(sort(rules_1,by=c(&quot;confidence&quot;),decreasing = T)[2:6]) top5 . lhs rhs support confidence coverage lift count [1] {householder status=live with parents/family} =&gt; {income=$40,000+} 0.1009245 1 0.1009245 1 262 [2] {type of home=apartment} =&gt; {income=$40,000+} 0.1359784 1 0.1359784 1 353 [3] {dual incomes=no} =&gt; {income=$40,000+} 0.2014638 1 0.2014638 1 523 [4] {marital status=single} =&gt; {income=$40,000+} 0.2087827 1 0.2087827 1 542 [5] {householder status=rent} =&gt; {income=$40,000+} 0.2538521 1 0.2538521 1 659 . NULL . . 2 . (a) . set.seed(1234) data2 &lt;- read_csv(&quot;sample_DT.csv&quot;) #glimpse(data2) data2$DEFECT_TYPE &lt;- as.factor(data2$DEFECT_TYPE) train &lt;- data2 %&gt;% sample_frac(0.7) test &lt;- setdiff(data2,train) . Rows: 10000 Columns: 21 -- Column specification Delimiter: &#34;,&#34; chr (1): DEFECT_TYPE dbl (20): ValueG, DIFF_BRG, ValueR, DEFECT_RBG, PKR.B, MinG, DEFECT_SIZE_Y, ... i Use `spec()` to retrieve the full column specification for this data. i Specify the column types or set `show_col_types = FALSE` to quiet this message. . (b) . tree.fit &lt;- rpart(DEFECT_TYPE~., control = rpart.control(maxdepth=5, minsplit=15),data=train) #tree.fit$cptable #rpart.plot(tree.fit) opt &lt;- which.min(tree.fit$cptable[,&quot;xerror&quot;]) cp &lt;- tree.fit$cptable[opt,&quot;CP&quot;] #plotcp(tree.fit) prune.c &lt;- prune(tree.fit,cp=cp) rpart.plot(prune.c,extra=2) . (c) . bagg.fit&lt;- ipredbagg(train$DEFECT_TYPE, train[,-21], nbagg=1000, coob=T) . (d) . train_1 &lt;- train unique(train_1$DEFECT_TYPE) train_1$DEFECT_TYPE &lt;- ifelse(train_1$DEFECT_TYPE==&quot;G&quot;,1,0) test_1 &lt;- test test_1$DEFECT_TYPE &lt;- ifelse(test_1$DEFECT_TYPE==&quot;G&quot;,1,0) boost.fit &lt;- gbm(DEFECT_TYPE~., data=train_1, distribution=&#39;bernoulli&#39;, interaction.depth = 5, n.trees=1000) . (e) . rf.fit &lt;- randomForest(DEFECT_TYPE ~., data = train, ntree = 50, mtry = 6, importance=T) . (f) .",
            "url": "https://gangcheol.github.io/data-mining/2022/06/23/%EB%8D%B0%EB%A7%88(Final-HW).html",
            "relUrl": "/2022/06/23/%EB%8D%B0%EB%A7%88(Final-HW).html",
            "date": " • Jun 23, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "현재 날짜와 시간을 파악해보기",
            "content": "&quot;ADP&quot; . toc:true- branch: master | badges: true | comments: true | author: 이강철 | categories: [python] | hide :false | published: true | . today &lt;- Sys.Date() today . 2021-12-05 class(today) . &#39;Date&#39; time &lt;- Sys.time() time . [1] &#34;2021-12-05 14:46:38 KST&#34; . class(time) . &lt;ol class=list-inline&gt;&#39;POSIXct&#39; | &#39;POSIXt&#39; | &lt;/ol&gt; 각각의 형식을 보면 생성된 객체들의 특성을 파악할 수 있다. | . unclass() . unclass()는 데이터의 클래스 속성을 제거해주는 기능을 함 | . unclass 함수를 이용해 today, time 의 내부적으로 저장하고 있는 값을 확인해보자. | . unclass(today) . 18966 today : 1970년 1월 1일 이후로 경과한 일 수를 의미함. | . unclass(time) . 1638683198.04487 1970년 1월 1일 00:00:00 이후로 경과한 초 수를 의미함. | . as.POSIXct(unclass(time),origin=&quot;1970-01-01&quot;) . [1] &#34;2021-12-05 14:46:38 KST&#34; . unclass(time)은 1970년 1월 1일 이후로 경과한 초 수를 의미하므로, origin 인자 값으로 1970-01-01을 지정 | . Sys.time &#54632;&#49688;&#47484; &#51060;&#50857;&#54616;&#50668; &#54788;&#51116; &#49884;&#44036;&#51012; &#44396;&#54616;&#44256; &#51060;&#47492; time &#48320;&#49688;&#50640; &#51200;&#51109; &#54980;, time&#51012; POSIXlt &#54805;&#49885;&#51004;&#47196; &#48320;&#54872; &#46244; &#49884;&#44036;&#51221;&#48372; &#52628;&#52636; . POSIXlt : 연도, 월, 일, 시, 분, 초를 포함하는 9개의 정보를 리스트에 저장 | . time &lt;- as.POSIXlt(Sys.time()) . year에는 1900년 이후의 경과년도수가 저장되어있음 | . time$year+1900 . 2021 month : 월 정보는 0-11값으로 저장되어 있기 때문에 1을 더함 | . time$mon+1 . 12 mday : 1-31 일 | . time$mday . 5 wday : 0-6 -&gt; 요일을 나타내며, 0은 일요일 | . time$wday . 0 그 밖에 sec, min, hour 이 있음 | . &#45216;&#51676; &#54364;&#49884;&#54805;&#49885; &#48320;&#44221; . now &lt;- Sys.time() . class(now) . &lt;ol class=list-inline&gt;&#39;POSIXct&#39; | &#39;POSIXt&#39; | &lt;/ol&gt; %u : 요일로 1-7을 나타냄 1은 월요일 | . format(now, &quot;%y-%m-%d %H:%M:%S:%u&quot;) . &#39;21-12-05 15:04:22:7&#39; class(format(now, &quot;%y-%m-%d %H:%M:%S:%u&quot;) ) . &#39;character&#39; &quot;20200101&quot; 이라는 문자열을 Date 형식으로 변경 후, date 변수에 저장하여 class를 확인해보자. | . date &lt;- as.Date(&quot;20200101&quot;,format = &quot;%Y%m%d&quot;) . class(date) . &#39;Date&#39; &#45216;&#51676; &#45936;&#51060;&#53552;&#51032; &#50672;&#49328; . &#54788;&#51116; &#45216;&#51088;&#47196;&#48512;&#53552; 100&#51068; &#54980;&#51032; &#45216;&#51676; &#44396;&#54616;&#44592; . Sys.Date() + 100 . 2022-03-15 &quot;2020-01-01&quot;&#47196; &#54364;&#54788;&#46108; &#45936;&#51060;&#53552;&#47196;&#48512;&#53552; 365&#51068; &#54980; &#51032; &#45216;&#51676; &#44396;&#54616;&#44592; . as.Date(&quot;2020-01-01&quot;,format=&quot;%Y-%m-%d&quot;) + 365 . 2020-12-31 &quot;1990-01-01&quot;&#44284; &quot;2025-01-01&quot; &#49324;&#51060;&#51032; &#51068; &#49688; &#44396;&#54616;&#44592; . as.Date(&quot;2025-01-01&quot;)-as.Date(&quot;1990-01-01&quot;) . Time difference of 12784 days . &#44396;&#54620; &#51068;&#49688;&#47564; &#50508;&#44256;&#49910;&#45796;&#47732;? . as.numeric(as.Date(&quot;2025-01-01&quot;)-as.Date(&quot;1990-01-01&quot;)) . 12784 difftime(&quot;2025-01-01&quot;,&quot;1990-01-01&quot;) . Time difference of 12784 days . &#49884;&#44036; &#52264;&#51060;&#47484; &#44396;&#54624; &#44221;&#50864;? . as.difftime(&quot;09:40:00&quot;)-as.difftime(&quot;18:30:00&quot;) . Time difference of -8.833333 hours .",
            "url": "https://gangcheol.github.io/data-mining/2022/06/23/%EB%82%A0%EC%A7%9C-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%A0%84%EC%B2%98%EB%A6%AC.html",
            "relUrl": "/2022/06/23/%EB%82%A0%EC%A7%9C-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%A0%84%EC%B2%98%EB%A6%AC.html",
            "date": " • Jun 23, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "결측치",
            "content": "&quot;ADP&quot; . toc:true- branch: master | badges: true | comments: true | author: 이강철 | categories: [python] | hide :false | published: true | . is.na(x) : 벡터에서 결측가 있는 경우 True를 반환 | . complete.cases(x) : 해당 데이터프레임에서 어떤 객체가 가지는 변수 중에 한 개라도 NA가 있을 경우 False 를 뱉음 | . R&#49892;&#49845; . Ozone 변수에 존재하는 na의 개수 산출 | . sum(is.na(airquality$Ozone)) . 37 table(is.na(airquality$Ozone)) . FALSE TRUE 116 37 . apply함수를 이용하여 각 변수의 na값이 몇 개가 있는지 확인 | . apply(airquality,2,function(x) sum(is.na(x))) . &lt;dl class=dl-inline&gt;Ozone37Solar.R7Wind0Temp0Month0Day0&lt;/dl&gt; complete.case 함수를 이용하여 airquality 데이터에서 na값이 하나라도 존재하는 행들을 air_na 변수에 저장하고, na값을 하나도 가지지 않는 행들을 air_com 변수에 저장하기. | . air_na &lt;- airquality[!complete.cases(airquality),] . head(air_na) . A data.frame: 6 × 6 OzoneSolar.RWindTempMonthDay . &lt;int&gt;&lt;int&gt;&lt;dbl&gt;&lt;int&gt;&lt;int&gt;&lt;int&gt; . 5NA | NA | 14.3 | 56 | 5 | 5 | . 628 | NA | 14.9 | 66 | 5 | 6 | . 10NA | 194 | 8.6 | 69 | 5 | 10 | . 11 7 | NA | 6.9 | 74 | 5 | 11 | . 25NA | 66 | 16.6 | 57 | 5 | 25 | . 26NA | 266 | 14.9 | 58 | 5 | 26 | . air_com &lt;- airquality[complete.cases(airquality),] . head(air_com) . A data.frame: 6 × 6 OzoneSolar.RWindTempMonthDay . &lt;int&gt;&lt;int&gt;&lt;dbl&gt;&lt;int&gt;&lt;int&gt;&lt;int&gt; . 141 | 190 | 7.4 | 67 | 5 | 1 | . 236 | 118 | 8.0 | 72 | 5 | 2 | . 312 | 149 | 12.6 | 74 | 5 | 3 | . 418 | 313 | 11.5 | 62 | 5 | 4 | . 723 | 299 | 8.6 | 65 | 5 | 7 | . 819 | 99 | 13.8 | 59 | 5 | 8 | . &#44208;&#52769;&#52824; &#45824;&#52824;&#48277; . completes analysis : 결측값이 존재하는 행을 삭제 | . 평균, 중앙값, 최빈값, nearest neighbor 방법 등이 있음 | . R &#49892;&#49845; . airquality의 Ozone 변수 값이 존재하지 않는 경우, 해당 변수를 평균값으로 대체하자. | . airquality$Ozone[is.na(airquality$Ozone)] &lt;- mean(airquality$Ozone, na.rm=T) . sum(is.na(airquality$Ozone)) . 0 DMwR &#54056;&#53412;&#51648; . library(DMwR2) . air_before &lt;- airquality . air_after &lt;- centralImputation(airquality) ## 결측치를 중앙값으로 대치 . na_indx &lt;- which(!complete.cases(airquality)) ## Na인덱스를 추출 . head(air_before[na_indx,]) . A data.frame: 6 × 6 OzoneSolar.RWindTempMonthDay . &lt;dbl&gt;&lt;int&gt;&lt;dbl&gt;&lt;int&gt;&lt;int&gt;&lt;int&gt; . 542.12931 | NA | 14.3 | 56 | 5 | 5 | . 628.00000 | NA | 14.9 | 66 | 5 | 6 | . 11 7.00000 | NA | 6.9 | 74 | 5 | 11 | . 2742.12931 | NA | 8.0 | 57 | 5 | 27 | . 9678.00000 | NA | 6.9 | 86 | 8 | 4 | . 9735.00000 | NA | 7.4 | 85 | 8 | 5 | . head(air_after) . A data.frame: 6 × 6 OzoneSolar.RWindTempMonthDay . &lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;int&gt;&lt;int&gt;&lt;int&gt; . 141.00000 | 190 | 7.4 | 67 | 5 | 1 | . 236.00000 | 118 | 8.0 | 72 | 5 | 2 | . 312.00000 | 149 | 12.6 | 74 | 5 | 3 | . 418.00000 | 313 | 11.5 | 62 | 5 | 4 | . 542.12931 | 205 | 14.3 | 56 | 5 | 5 | . 628.00000 | 205 | 14.9 | 66 | 5 | 6 | . 아래의 값을 살펴본 결과 결측치가 중앙값으로 잘 대치 되었음을 확인하였다. | . median(airquality$Solar.R,na.rm=T) . 205 k최근접 이웃 알고리즘을 이용하여 na값을 대치해보기 | . apply(knnImputation(air_before,k=3),2,function(x) sum(is.na(x))) . &lt;dl class=dl-inline&gt;Ozone0Solar.R0Wind0Temp0Month0Day0&lt;/dl&gt;",
            "url": "https://gangcheol.github.io/data-mining/2022/06/23/%EA%B2%B0%EC%B8%A1%EC%B9%98(1205).html",
            "relUrl": "/2022/06/23/%EA%B2%B0%EC%B8%A1%EC%B9%98(1205).html",
            "date": " • Jun 23, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "상관분석",
            "content": "$H_ 0 : $ 변수간에는 상관관계가 없다( 상관계수 $= 0$) . $H_1 : $ 변수간에는 상관관계가 있다.( 상관계수 $ neq 0$ ) . 피어슨 상관계수 두 연속형 자료가 모두 정규성을 따른다는 가정하에 선형적 상관관계를 측정 | . | . 스피어만 상관계수 데이터가 정규성을 만족하지 않거나 순위 및 순서 형태로 주어지는 경우 사용 | 피어슨 상관계수와 달리 비선형 관계의 연관성을 파악할 수 있다. | 비모수적 방법 | . | . 켄달의 순위상관계수 $X_i$가 커짐에 따라 $Y_i$도 커질 경우 부합, 작아질 경우 비부합이라고 본다. | 전체 데이터에서 비부합쌍에 대한 부합쌍의 비율로 상관계수를 산출한다. | 순위상관계수가 -1 일 경우 비부합쌍의 비율이 100%, 0일 경우 두 변수 $X,Y$는 상관성이 없음을 의미한다. | . | . $ divideontimes$ 원래의 경우 독립이면 상관계수는 0이지만, 이것에 대한 역은 반드시 성립하지 않는다. . R &#49892;&#49845; . library(tidyverse) . data(&quot;airquality&quot;) . air &lt;- airquality %&gt;% select(-c(Day,Month)) . air_cor &lt;- cor(air,use=&quot;pairwise.complete.obs&quot;,method=&quot;pearson&quot;) . use everything : 결측값 존재 시 NA출력 | all.obs : 결측값 존재 시 오류 메시지 출력 | complete.obs : 변수별로 결측값을 제외하고 상관계수 계산 | pairwise.complete.obs : 모든 변수 쌍에서 결측값이 없는 데이터들에 대해 상관계수 계산 | . | . library(corrplot) . testRes &lt;- cor.mtest(air,method=&quot;pearson&quot;) . options(repr.plot.res=200,repr.plot.height=5,repr.plot.width=10) corrplot(air_cor,diag=F,type=&quot;lower&quot;,p.mat=testRes$p, method=&quot;circle&quot;,number.cex=1.5,addCoef.col=&quot;black&quot;) . 위 그래프를 해석하면 Wind와 Solar.R간에는 상관관계가 없다고 해석할 수 있다. 즉 독립인다고 할 수 있다. | . cor.test(air$Wind,air$Solar.R) ##실제 검정 결과도 동일하다. . Pearson&#39;s product-moment correlation data: air$Wind and air$Solar.R t = -0.6826, df = 144, p-value = 0.496 alternative hypothesis: true correlation is not equal to 0 95 percent confidence interval: -0.2172359 0.1066406 sample estimates: cor -0.05679167 .",
            "url": "https://gangcheol.github.io/data-mining/2022/06/23/%EC%83%81%EA%B4%80%EB%B6%84%EC%84%9D.html",
            "relUrl": "/2022/06/23/%EC%83%81%EA%B4%80%EB%B6%84%EC%84%9D.html",
            "date": " • Jun 23, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "(4주차) 과제",
            "content": "1&#48264; . 설명변수가 1개($X$)이고, 반응변수가 1개($Y$)인데이터를 가지고 있다고 하자. $(n = 100)$ 그리고 다음의 두 모형(linear regression, cubic regression)을 적합시키려고 한다. . $$Y = beta_{0}+ beta_{1}X + varepsilon quad quad quad quad quad quad quad quad (1)$$ . $$Y = beta_{0}+ beta_{1}X + beta_{2}X^2+ beta_{3}X^3+ varepsilon quad quad (2)$$ . (a) . 실제 $X,Y$가 선형(linear)관계가 있다고 가정 하자. 모델 (1),(2)의 $SSE$(잔차제곱합)의 크기를 비교할 수 있는지 설명하여라. . Solution . 비교할 수 있다. train sse의 경우 과적합된 cubic regreesion이 낮은 train sse를 보일 것이다. 그러나 선형관계라는 가정이 존재하기 때문에 test sse는 linear regression 이 더 낮은 수치를 보일 것이다. | . (b) . 실제 $X, Y$ 가 비선형(non-linear)관계가 있다고 가정 하자. 대신 실제 모형에 대한 정보는 없다. 모델 (1),(2)의 $SSE$(잔차제곱합)의 크기를 비교할 수 있는지 설명하여라. . Solution . 비교할 수 없다. (a) 번의 경우 두 변수는 선형관계라는 모형 정보가 존재했다. 그러나 (b)번의 경우 모형에 대한 정보가 없기 때문에 test 데이터에 대한 sse로 모형을 비교할 수 없다. | . . 2&#48264; . ’Auto.csv’ 데이터를 이용하여 단순선형 회귀 모형을 적합한다. . (자료형 변환 후 결측치 제거) . library(tidyverse) data &lt;- read_csv(&quot;Auto(1).csv&quot;) data$horsepower &lt;- as.numeric(data$horsepower) glimpse(data) . -- Attaching packages - tidyverse 1.3.1 -- v ggplot2 3.3.5 v purrr 0.3.4 v tibble 3.1.6 v dplyr 1.0.8 v tidyr 1.2.0 v stringr 1.4.0 v readr 2.1.2 v forcats 0.5.1 -- Conflicts - tidyverse_conflicts() -- x dplyr::filter() masks stats::filter() x dplyr::lag() masks stats::lag() . Error: &#39;Auto(1).csv&#39; does not exist in current working directory (&#39;C:/Users/rkdcj/데이터마이닝특강&#39;). Traceback: 1. read_csv(&#34;Auto(1).csv&#34;) 2. vroom::vroom(file, delim = &#34;,&#34;, col_names = col_names, col_types = col_types, . col_select = { . { . col_select . } . }, id = id, .name_repair = name_repair, skip = skip, n_max = n_max, . na = na, quote = quote, comment = comment, skip_empty_rows = skip_empty_rows, . trim_ws = trim_ws, escape_double = TRUE, escape_backslash = FALSE, . locale = locale, guess_max = guess_max, show_col_types = show_col_types, . progress = progress, altrep = lazy, num_threads = num_threads) 3. vroom_(file, delim = delim %||% col_types$delim, col_names = col_names, . col_types = col_types, id = id, skip = skip, col_select = col_select, . name_repair = .name_repair, na = na, quote = quote, trim_ws = trim_ws, . escape_double = escape_double, escape_backslash = escape_backslash, . comment = comment, skip_empty_rows = skip_empty_rows, locale = locale, . guess_max = guess_max, n_max = n_max, altrep = vroom_altrep(altrep), . num_threads = num_threads, progress = progress) 4. (function (path, write = FALSE) . { . if (is.raw(path)) { . return(rawConnection(path, &#34;rb&#34;)) . } . if (!is.character(path)) { . return(path) . } . if (is_url(path)) { . if (requireNamespace(&#34;curl&#34;, quietly = TRUE)) { . con &lt;- curl::curl(path) . } . else { . rlang::inform(&#34;`curl` package not installed, falling back to using `url()`&#34;) . con &lt;- url(path) . } . ext &lt;- tolower(tools::file_ext(path)) . return(switch(ext, zip = , bz2 = , xz = { . close(con) . stop(&#34;Reading from remote `&#34;, ext, &#34;` compressed files is not supported, n&#34;, . &#34; download the files locally first.&#34;, call. = FALSE) . }, gz = gzcon(con), con)) . } . p &lt;- split_path_ext(basename(path)) . if (write) { . path &lt;- normalizePath(path, mustWork = FALSE) . } . else { . path &lt;- check_path(path) . } . if (rlang::is_installed(&#34;archive&#34;)) { . formats &lt;- archive_formats(p$extension) . extension &lt;- p$extension . while (is.null(formats) &amp;&amp; nzchar(extension)) { . extension &lt;- split_path_ext(extension)$extension . formats &lt;- archive_formats(extension) . } . if (!is.null(formats)) { . p$extension &lt;- extension . if (write) { . if (is.null(formats[[1]])) { . return(archive::file_write(path, filter = formats[[2]])) . } . return(archive::archive_write(path, p$path, format = formats[[1]], . filter = formats[[2]])) . } . if (is.null(formats[[1]])) { . return(archive::file_read(path, filter = formats[[2]])) . } . return(archive::archive_read(path, format = formats[[1]], . filter = formats[[2]])) . } . } . if (!write) { . compression &lt;- detect_compression(path) . } . else { . compression &lt;- NA . } . if (is.na(compression)) { . compression &lt;- tools::file_ext(path) . } . if (write &amp;&amp; compression == &#34;zip&#34;) { . stop(&#34;Can only read from, not write to, .zip&#34;, call. = FALSE) . } . switch(compression, gz = gzfile(path, &#34;&#34;), bz2 = bzfile(path, . &#34;&#34;), xz = xzfile(path, &#34;&#34;), zip = zipfile(path, &#34;&#34;), . if (!has_trailing_newline(path)) { . file(path) . } else { . path . }) . })(&#34;Auto(1).csv&#34;) 5. check_path(path) 6. stop(&#34;&#39;&#34;, path, &#34;&#39; does not exist&#34;, if (!is_absolute_path(path)) { . paste0(&#34; in current working directory (&#39;&#34;, getwd(), &#34;&#39;)&#34;) . }, &#34;.&#34;, call. = FALSE) . summary(data) . mpg cylinders displacement horsepower weight Min. : 9.00 Min. :3.000 Min. : 68.0 Min. : 46.0 Min. :1613 1st Qu.:17.50 1st Qu.:4.000 1st Qu.:104.0 1st Qu.: 75.0 1st Qu.:2223 Median :23.00 Median :4.000 Median :146.0 Median : 93.5 Median :2800 Mean :23.52 Mean :5.458 Mean :193.5 Mean :104.5 Mean :2970 3rd Qu.:29.00 3rd Qu.:8.000 3rd Qu.:262.0 3rd Qu.:126.0 3rd Qu.:3609 Max. :46.60 Max. :8.000 Max. :455.0 Max. :230.0 Max. :5140 NA&#39;s :5 acceleration year origin name Min. : 8.00 Min. :70.00 Min. :1.000 Length:397 1st Qu.:13.80 1st Qu.:73.00 1st Qu.:1.000 Class :character Median :15.50 Median :76.00 Median :1.000 Mode :character Mean :15.56 Mean :75.99 Mean :1.574 3rd Qu.:17.10 3rd Qu.:79.00 3rd Qu.:2.000 Max. :24.80 Max. :82.00 Max. :3.000 . data &lt;- na.omit(data) . (a) . 반응변수 mpg, 설명변수는 horsepower로 하는 단순선형회귀모형을 적합 시킨 후 summary() 함수의 결과 확인하고 다음의 물음에 답하여라. . fit1 &lt;- lm(mpg~horsepower,data) summary(fit1) . Call: lm(formula = mpg ~ horsepower, data = data) Residuals: Min 1Q Median 3Q Max -13.5710 -3.2592 -0.3435 2.7630 16.9240 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 39.935861 0.717499 55.66 &lt;2e-16 *** horsepower -0.157845 0.006446 -24.49 &lt;2e-16 *** Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 4.906 on 390 degrees of freedom Multiple R-squared: 0.6059, Adjusted R-squared: 0.6049 F-statistic: 599.7 on 1 and 390 DF, p-value: &lt; 2.2e-16 . (i) . 두 변수 사이에 관계가 있는가? . Solution . t 통계량에 근거한 p-value 값을 살펴본 결과 두 변수는 통계적으로 유의미한 선형관계에 있다 . (ii) . 두 변수 사이의 관계는 얼마나 강한가? . Solution . summary(fit1) . Call: lm(formula = mpg ~ horsepower, data = data) Residuals: Min 1Q Median 3Q Max -13.5710 -3.2592 -0.3435 2.7630 16.9240 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 39.935861 0.717499 55.66 &lt;2e-16 *** horsepower -0.157845 0.006446 -24.49 &lt;2e-16 *** Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 4.906 on 390 degrees of freedom Multiple R-squared: 0.6059, Adjusted R-squared: 0.6049 F-statistic: 599.7 on 1 and 390 DF, p-value: &lt; 2.2e-16 . horsepower 변수가 1씩 증가할 때 mpg 변수는 -0.1575 만큼 감소한다. . (iii) . 두 변수는 음의 관계가 있는가? 양의 관계가 있는가? . Solution . 추정된 $ beta_1$을 보았을 때 음의 관계이다. . (iv) . horsepower의 값이 98일 때, mpg의 예측값은 무엇인가 95% 신뢰구간은 무엇인가? . Solution . test &lt;- data.frame(horsepower=c(98)) predict(fit1,test,interval = &quot;confidence&quot;) . A matrix: 1 × 3 of type dbl fitlwrupr . 124.46708 | 23.97308 | 24.96108 | . (b) . 설명변수와 반응변수의 산점도를 그리고, 회귀직선을 추가하여라. (abline() 사용) . Solution . fit1$coefficients . . &lt;dl class=dl-inline&gt;(Intercept)39.9358610211705horsepower-0.157844733353654&lt;/dl&gt; options(repr.plot.res=150,repr.plot.width=10) plot(data$horsepower,data$mpg,xlab=&quot;horsepower&quot;,ylab=&quot;mpg&quot;,main = &quot;plot with abline&quot;) abline(a = fit1$coefficients[1],b=fit1$coefficients[2],col=&quot;red&quot;,lwd=2) . . . 3&#48264; . 이 문제는 다중공선성(collinearity)에 관련한 것이다. . (a) . R . set.seed(1) x1 = runif(100) x2 = 0.5*x1 + rnorm(100)/10 y = 2 + 2*x1+0.3*x2 + rnorm(100) . 마지막 줄이 두개의 설명변수를 이용한 중회귀모형이다. 회귀모형을 쓰시오. ($ beta$ 등을 이용하여) . Solution . set.seed(1) x1 = runif(100) x2 = 0.5*x1 + rnorm(100)/10 y = 2 + 2*x1+0.3*x2 + rnorm(100) . . $$y= beta_1 x_1 + beta_2 x_2 + varepsilon, quad varepsilon sim N(0, sigma^2)$$ . (b) . 두 설명변수 $x_1$과 $x_2$ 사이에 상관관계(correlation)이 있는가? 산점도를 그려서 확인하여라 . Solution . options(repr.plot.res=200) plot(x1,x2,main = &quot;x1, x2 correlation&quot;) abline(a=-0.1,b=1,col=&quot;red&quot;) . . 산점도를 그려본 결과 $x_1,x_2$의 분포가 직선 형태의 가까운 형태이다. 따라서 두 변수 사이에 강한 상관성이 있어보인다. . (c) . 생성된 데이터를 이용하여 (a) 모형의 회귀계수를 추정하여라. 실제 회귀계수와 추정된 회귀계수와 비교하여라. $H_0 : beta_1 = 0$을 기각할 수 있는가? $H_1 : beta_2 = 0$을 기각할 수 있는가? . Solution . fit2 &lt;- lm(y~x1+x2) summary(fit2) . . Call: lm(formula = y ~ x1 + x2) Residuals: Min 1Q Median 3Q Max -2.8311 -0.7273 -0.0537 0.6338 2.3359 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 2.1305 0.2319 9.188 7.61e-15 *** x1 1.4396 0.7212 1.996 0.0487 * x2 1.0097 1.1337 0.891 0.3754 Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 1.056 on 97 degrees of freedom Multiple R-squared: 0.2088, Adjusted R-squared: 0.1925 F-statistic: 12.8 on 2 and 97 DF, p-value: 1.164e-05 . t 통계량에 근거한 p-value 값을 보았을 때 $H_0 : beta_1 = 0$ 이라는 기각가능하나, $H_0 : beta_2 = 0$ 기각하지 못한다 . (d) . 이번에는 $x_1$만을 이용한 단순선형회귀 모형을 적합하여라. 결과를 분석하여라. $H_0 : beta_1 = 0$을 기각할 수 있는가 . Solution . fit3 &lt;- lm(y~x1) summary(fit3) . . Call: lm(formula = y ~ x1) Residuals: Min 1Q Median 3Q Max -2.89495 -0.66874 -0.07785 0.59221 2.45560 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 2.1124 0.2307 9.155 8.27e-15 *** x1 1.9759 0.3963 4.986 2.66e-06 *** Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 1.055 on 98 degrees of freedom Multiple R-squared: 0.2024, Adjusted R-squared: 0.1942 F-statistic: 24.86 on 1 and 98 DF, p-value: 2.661e-06 . 기각 가능하다. . (e) . 이번에는 $x_2$만을 이용한 단순선형회귀 모형을 적합하여라. 결과를 분석하여라. $H_0 : beta_2 = 0$을 기각할 수 있는가? . Solution . fit4 &lt;- lm(y~x2) summary(fit4) . . Call: lm(formula = y ~ x2) Residuals: Min 1Q Median 3Q Max -2.62687 -0.75156 -0.03598 0.72383 2.44890 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 2.3899 0.1949 12.26 &lt; 2e-16 *** x2 2.8996 0.6330 4.58 1.37e-05 *** Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 1.072 on 98 degrees of freedom Multiple R-squared: 0.1763, Adjusted R-squared: 0.1679 F-statistic: 20.98 on 1 and 98 DF, p-value: 1.366e-05 . 기각 가능하다 . (f) . (c)-(e)의 결과가 서로 모순되는가? 설명하여라. . Solution . 결론 : 모순이 아니다 . 현재 $x_1,x_2$를 독립변수로 $y$를 반응변수로 하여 적합한 중회귀모형 모델은 $ beta_1 neq0$, $ beta_2=0$이다. . 그러나 $x_1,x_2$ 각각의 경우를 $y$변수에 적합한 결과 $ beta_i neq 0, quad i= {1,2 }$를 보였다. . 현재 $x_2= 0.5 times x_1 + varepsilon$의 형태이다. . 즉 $x_2$와 $x_1$을 이미 선형관계를 전제했기 때문에 두 변수를 이용하여 $y$변수에 적합시킬 경우 다중공선성의 문제로 인하여 위와 같은 현상이 발생한다. 따라서 이는 모순이 아닌 합당한 결과이다. $x_1,x_2$는 선형의 관계이므로 주성분 분석을 이용하여 하나의 변수로 통합시킬 수 있다. . (g) . 새로운 데이터가 관측되었다고 하자.(이 데이터는 잘못 측정된 것이다.) . R . x1 &lt;- c(x1,0.1) x2 &lt;- c(x2,0.8) ㅛ &lt;- c(y,6) . 추가된 데이터를 이용하여 (c)-(e)를 다시 적합하여라. 결과가 어떻게 달라졌는가? 각 모형에서 새로 운 데이터는 이상점인가?(잔차가 기존에 있는 데이터에 비해 많이 큰가?) 아니면 영향점인가?(추가된 데이터로 인해 회귀계수의 값이 많이 바뀌었는가?) 설명하여라. . Solution . x1 &lt;- c(x1,0.1) x2 &lt;- c(x2,0.8) y &lt;- c(y,6) . . f1 &lt;- lm(y~x1+x2) f2 &lt;- lm(y~x1) f3 &lt;- lm(y~x2) par(mfrow=c(3,2)) plot(fit2,1,main = &quot;추가전 y ~ x1+x2&quot;) plot(f1,1,main = &quot;추가후 y ~ x1+x2&quot;) plot(fit3,1,main = &quot;추가전 y ~ x1&quot;) plot(f2,1,main = &quot;추가후 y ~ x1&quot;) plot(fit4,1,main = &quot;추가전 y ~ x2&quot;) plot(f3,1,main = &quot;추가후 y ~ x2&quot;) . . 해설 1 : 데이터 추가 후 잔차를 살펴본 결과 이상치로 판단되지 않는다. | . 해설 2 : 데이터 추가 후 의 변화를 살펴본 결과 의 경우 데이터 추가 후 , 의 감소량이 눈에 띈다. 따라서 추가된 데이터는 영향치로 판단된다. | . Exercises for Logistic Regression . 1&#48264; . 두개의 설명변수 (X1 = 공부시간, X2 = 학부평점)를 이용하여 A학점을 받을 확률을 예측하기 위해 로지스틱 회귀모형을 적합하였다. 추정된 회귀계수는 $ beta_0 = −6, , beta_1 = 0.05, , beta_2 = 1$이다. . (a) . 40시간 공부하고, 평점이 3.5인 학생이 A를 받았을 확률을 예측하여라. . Solution . $$P(X) = frac {e^{ beta_0+ beta_1x_1+ beta_2 x_2}}{1+e^{ beta_0+ beta_1 x_1 + beta_2 x_2}}$$ . $$ therefore quad P(X | x_1 = 40, x_2 = 3.5) = 0.377$$ . (b) . 평점이 3.5인 학생은 얼마나 공부를 해야 A를 받을 확률이 50%를 넘을 것인가? . Solution . x1 &lt;- 40:100 x2 &lt;- 3.5 P_x &lt;- (exp(-6+0.05*x1 + x2))/(1+exp(-6+0.05*x1 + x2)) d &lt;- tibble(x1,P_x) %&gt;% mutate(label = ifelse(P_x&gt;0.5,&quot;A&quot;,&quot;not A&quot;)) d %&gt;% ggplot(aes(x=x1,y=P_x,color=label)) + geom_label(aes(label= x1)) + ggtitle(&quot;공부시간에 따른 학점변화&quot;) + theme( plot.title = element_text(family = &quot;serif&quot;, face = &quot;bold&quot;, hjust = 0.5, size = 20, color = &quot;darkblue&quot;), axis.text.x=element_text(size=15), axis.text.y=element_text(size=15), axis.title.x = element_text(size=18, hjust=0.5,color=&quot;darkblue&quot;, family = &quot;serif&quot;,face = &quot;bold&quot;), axis.title.y = element_text(size=18, hjust=0.5,color=&quot;darkblue&quot;, family = &quot;serif&quot;,face = &quot;bold&quot;), ) . . 2&#48264; . 다음은 odds에 관한 문제이다. . (a) . 신용카드결재 문제에서 결재를 하지 못하는 경우(default)에 대한 odds가 0.37인 사람들이 실제로 defalut할 확률은 평균적으로 얼마인가? . Solution . $$Odds = frac{p}{1-p} to E(Y=default| X)=p = frac {Odds}{1+Odds}$$ . $$p = frac{0.37}{1+0.37}= 0.270073$$ . (b) . 어떤 개인이 default할 확률이 16% 라고 하자. 그 사람이 default할 odds는 얼마인가? . Solution . $$Odds = frac {0.16}{1-0.16} = 0.1904$$ .",
            "url": "https://gangcheol.github.io/data-mining/2022/03/30/(4%EC%A3%BC%EC%B0%A8)-%EA%B3%BC%EC%A0%9C.html",
            "relUrl": "/2022/03/30/(4%EC%A3%BC%EC%B0%A8)-%EA%B3%BC%EC%A0%9C.html",
            "date": " • Mar 30, 2022"
        }
        
    
  
    
        ,"post9": {
            "title": "(1주차) 선형회귀분석",
            "content": "&#49440;&#54805;&#54924;&#44480;&#48516;&#49437; &#44592;&#52488; . 변수들간의 인과관계를 밝히고 모형을 적합하여 관심 있는 변수를 예측하거나 추론하기 위해 사용하는 분석기법 | . 선형회귀분석의 가정 오차의 등분산성 | 오차의 독립성 | 오차의 정규성 : Q-Q plot, Kolmogorov-Smirnov 검정, Shapiro-Wilk 검정을 확인하여 정규성을 확인한다. | . | . &#54924;&#44480;&#48516;&#49437; &#49884; &#44160;&#53664;&#49324;&#54637; . 0. 회귀모형이 통계적으로 유의한가 확인 . 1. 모형 내의 개별 회귀계수에 대한 검정 . 2. 모형에 설명력 $R^2$값을 통해 확인, 독립변수의 수가 많아지면 $adj-R^2$ 값을 확인 . 3. 잔차 plot을 통해 모형의 진단 . 4. 다중공선성의 확인 (10이상이면 다중공선성이 존재한다고 판단.) $ to$ car 패키지의 vif 함수 이용 . 5. 잔차분석 . R&#49892;&#49845; - &#45800;&#49692;&#49440;&#54805;&#54924;&#44480;&#48516;&#49437; . Cars93 데이터의 엔진크기(EngineSize)를 독립변수, 가격(Price)를 종속변수로 선정하여 단순 선형회귀분석을 실시한 후, 추정된 회귀모형에 대해 해석해보자. . library(MASS) library(lmtest) ## 더비왓슨 테스트를 위함 library(tidyverse) select &lt;- dplyr::select . . fit1 &lt;- lm(Price~EngineSize,data=Cars93) summary(fit1) . . Call: lm(formula = Price ~ EngineSize, data = Cars93) Residuals: Min 1Q Median 3Q Max -13.684 -4.627 -1.795 2.592 39.429 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 4.6692 2.2390 2.085 0.0398 * EngineSize 5.5629 0.7828 7.107 2.59e-10 *** Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 7.789 on 91 degrees of freedom Multiple R-squared: 0.3569, Adjusted R-squared: 0.3499 F-statistic: 50.51 on 1 and 91 DF, p-value: 2.588e-10 . plot(Cars93$EngineSize,Cars93$Price,lwd=2) abline(a=coefficients(fit1)[2],b=coefficients(fit1)[1],col=&quot;red&quot;,lwd=2) . . par(mfrow=c(1,2)) plot(fit1,1); plot(fit1,2) . . shapiro.test(resid(fit1)) . . Shapiro-Wilk normality test data: resid(fit1) W = 0.85365, p-value = 3.886e-08 . dwtest(fit1,alternative=&quot;two.sided&quot;) . . Durbin-Watson test data: fit1 DW = 1.1716, p-value = 2.236e-05 alternative hypothesis: true autocorrelation is not 0 . 1. 모형과 추정된 회귀계수는 모두 통계적으로 유의하다. . 2. 결정계수값과 수정된 결정계수 값이 각각 0.3569, 0.3499 로 산출되었다. . 3. F-통계량의 근거한 p-value값을 보아도 생성된 모델은 통계적으로 유의하다. . 4. 잔차 plot 을 그려본 결과 오차항의 정규성과 독립성 가정이 위배된 것 같다. . * 실제로 test 결과 위배되었다는 결론이 통계적으로 유의미했다. . 5. 따라서 모형의 식별 단계로 돌아가 새로운 모형을 적합할 필요가 있어보인다. . test &lt;- Cars93 %&gt;% select(EngineSize) %&gt;% sample_n(5) . . predict(fit1,test,interval=&quot;none&quot;) ##점추정 . . &lt;dl class=dl-inline&gt;123.0268912710567216.9076569678407324.1394793261868425.8083614088821523.5831852986217&lt;/dl&gt; predict(fit1,test,interval=&quot;confidence&quot;) # 회귀계수에 대한 신뢰구간을 고려한 구간 predict(fit1,test,interval=&quot;prediction&quot;) # 회귀계수에 대한 신뢰구간과 오차항을 고려한 구간 . . A matrix: 5 × 3 of type dbl fitlwrupr . 123.02689 | 21.14536 | 24.90842 | . 216.90766 | 15.14623 | 18.66909 | . 324.13948 | 22.07834 | 26.20061 | . 425.80836 | 23.42653 | 28.19019 | . 523.58319 | 21.61595 | 25.55043 | . A matrix: 5 × 3 of type dbl fitlwrupr . 123.02689 | 7.441846 | 38.61194 | . 216.90766 | 1.336654 | 32.47866 | . 324.13948 | 8.531732 | 39.74723 | . 425.80836 | 10.155035 | 41.46169 | . 523.58319 | 7.987560 | 39.17881 | . R&#49892;&#49845; - &#51473;&#54924;&#44480;&#48516;&#49437; . iris 데이터를 사용 | . R에 lm함수는 범주형 변수를 자동으로 더미변수로 변환해줌 | . fit2 &lt;- lm(Petal.Length~.,data=iris) summary(fit2) . . Call: lm(formula = Petal.Length ~ ., data = iris) Residuals: Min 1Q Median 3Q Max -0.78396 -0.15708 0.00193 0.14730 0.65418 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -1.11099 0.26987 -4.117 6.45e-05 *** Sepal.Length 0.60801 0.05024 12.101 &lt; 2e-16 *** Sepal.Width -0.18052 0.08036 -2.246 0.0262 * Petal.Width 0.60222 0.12144 4.959 1.97e-06 *** Speciesversicolor 1.46337 0.17345 8.437 3.14e-14 *** Speciesvirginica 1.97422 0.24480 8.065 2.60e-13 *** Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.2627 on 144 degrees of freedom Multiple R-squared: 0.9786, Adjusted R-squared: 0.9778 F-statistic: 1317 on 5 and 144 DF, p-value: &lt; 2.2e-16 . dwtest(fit2,alternative=&quot;two.sided&quot;) . . Durbin-Watson test data: fit2 DW = 1.6772, p-value = 0.03042 alternative hypothesis: true autocorrelation is not 0 . shapiro.test(resid(fit2)) . . Shapiro-Wilk normality test data: resid(fit2) W = 0.99389, p-value = 0.78 . library(car) vif(fit2) . . A matrix: 4 × 3 of type dbl GVIFDfGVIF^(1/(2*Df)) . Sepal.Length 3.736705 | 1 | 1.933056 | . Sepal.Width 2.648127 | 1 | 1.627307 | . Petal.Width18.496973 | 1 | 4.300811 | . Species28.551416 | 2 | 2.311569 | . &#52572;&#51201;&#54924;&#44480;&#48169;&#51221;&#49885;&#51032; &#49440;&#53469; . &#44032;. &#45800;&#44228;&#51201; &#48320;&#49688;&#49440;&#53469;(Stepwise Variable Selection) . 1. 전진 선택법 (forward selection) : 절편만 있는 상수모형에서 시작하여 중요하다고 생각되는 설명변수부터 차례로 추가한다. . 2. 후진 제거법 (backward elimination) : 모든 독립변수를 포함한 모형에서 출발하여 종속변수에 가장 적은 영향을 주는 변수부터 하나씩 제거하면서 더 이상 제거할 변수가 없을 때의 모형을 선택한다. . 3. 단계적 방법 (stepwise method) : 전진선택법에 의해 변수를 추가하면서 새롭게 추가된 변수에 의해 기존 변수의 중요도가 약화되면 해당변수를 제거한다. . &#45208;. &#48268;&#51216;&#54868;&#46108; &#49440;&#53469;&#44592;&#51456; . 모형의 복잡도에 따라 벌점을 주는 방식으로 $AIC, BIC$ 값이 주로 사용된다. | . R&#49892;&#49845; : &#45796;&#51473;&#54924;&#44480;&#47784;&#54805; + &#48320;&#49688;&#49440;&#53469;&#48277; . fit3 &lt;- step(lm(Price~ EngineSize +Horsepower +RPM + Width + Length + Weight,Cars93),direction = &quot;both&quot;) summary(fit3) . . Start: AIC=322.11 Price ~ EngineSize + Horsepower + RPM + Width + Length + Weight Df Sum of Sq RSS AIC - EngineSize 1 1.69 2556.1 320.17 - RPM 1 19.71 2574.1 320.82 &lt;none&gt; 2554.4 322.11 - Length 1 119.55 2674.0 324.36 - Weight 1 209.73 2764.2 327.45 - Width 1 585.01 3139.4 339.29 - Horsepower 1 720.84 3275.3 343.22 Step: AIC=320.17 Price ~ Horsepower + RPM + Width + Length + Weight Df Sum of Sq RSS AIC - RPM 1 49.36 2605.5 319.95 &lt;none&gt; 2556.1 320.17 + EngineSize 1 1.69 2554.4 322.11 - Length 1 140.92 2697.0 323.16 - Weight 1 208.09 2764.2 325.45 - Width 1 593.56 3149.7 337.59 - Horsepower 1 1476.65 4032.8 360.57 Step: AIC=319.95 Price ~ Horsepower + Width + Length + Weight Df Sum of Sq RSS AIC &lt;none&gt; 2605.5 319.95 + RPM 1 49.36 2556.1 320.17 + EngineSize 1 31.34 2574.1 320.82 - Length 1 132.02 2737.5 322.54 - Weight 1 279.31 2884.8 327.42 - Width 1 562.10 3167.6 336.12 - Horsepower 1 1898.74 4504.2 368.86 . Call: lm(formula = Price ~ Horsepower + Width + Length + Weight, data = Cars93) Residuals: Min 1Q Median 3Q Max -14.956 -2.578 -0.182 2.114 28.448 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 53.005861 16.532269 3.206 0.00188 ** Horsepower 0.129653 0.016190 8.008 4.46e-12 *** Width -1.480623 0.339813 -4.357 3.56e-05 *** Length 0.152968 0.072440 2.112 0.03755 * Weight 0.007339 0.002389 3.071 0.00283 ** Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 5.441 on 88 degrees of freedom Multiple R-squared: 0.6965, Adjusted R-squared: 0.6827 F-statistic: 50.48 on 4 and 88 DF, p-value: &lt; 2.2e-16 .",
            "url": "https://gangcheol.github.io/data-mining/r/jupyter/2022/03/03/(1%EC%A3%BC%EC%B0%A8)-%EC%84%A0%ED%98%95%ED%9A%8C%EA%B7%80%EB%B6%84%EC%84%9D.html",
            "relUrl": "/r/jupyter/2022/03/03/(1%EC%A3%BC%EC%B0%A8)-%EC%84%A0%ED%98%95%ED%9A%8C%EA%B7%80%EB%B6%84%EC%84%9D.html",
            "date": " • Mar 3, 2022"
        }
        
    
  
    
        ,"post10": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://gangcheol.github.io/data-mining/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Github . github . Soundcloud . C.I.C . NLP . NLP . Data Mining . Data Mining . Bigdata Analysis . Bigdata Analysis .",
          "url": "https://gangcheol.github.io/data-mining/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://gangcheol.github.io/data-mining/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}