{
  
    
        "post0": {
            "title": "데이터마이닝 특강 4주차 과제",
            "content": "1&#48264; . 설명변수가 1개($X$)이고, 반응변수가 1개($Y$)인데이터를 가지고 있다고 하자. $(n = 100)$ 그리고 다음의 두 모형(linear regression, cubic regression)을 적합시키려고 한다. . $$Y = beta_{0}+ beta_{1}X + varepsilon$$ . $$Y = beta_{0}+ beta_{1}X + beta_{2}X^2+ beta_{3}X 3+ varepsilon$$ . (a) . 실제 $X,Y$가 선형(linear)관계가 있다고 가정 하자. 모델 (1),(2)의 SSE(잔차제곱합)의 크기를 비교할 수 있는지 설명하여라. . Solution . (b) . 실제 $X, Y$ 가 비선형(non-linear)관계가 있다고 가정 하자. 대신 실제 모형에 대한 정보는 없다. 모델 (1),(2)의 $SSE$(잔차제곱합)의 크기를 비교할 수 있는지 설명하여라. . Solution . . 2&#48264; . ’Auto.csv’ 데이터를 이용하여 단순선형 회귀 모형을 적합한다. . (a) . 반응변수 mpg, 설명변수는 horsepower로 하는 단순선형회귀모형을 적합 시킨 후 summary() 함수의 결과 확인하고 다음의 물음에 답하여라. . (i) . 두 변수 사이에 관계가 있는가? . Solution . (ii) . Solution . 두 변수 사이의 관계는 얼마나 강한가? . (iii) . Solution . horsepower의 값이 98일 때, mpg의 예측값은 무엇인가 95% 신뢰구간은 무엇인가? . (iv) . Solution . (b) . 설명변수와 반응변수의 산점도를 그리고, 회귀직선을 추가하여라. (abline() 사용) . Solution . . 3&#48264; . 이 문제는 다중공선성(collinearity)에 관련한 것이다. . (a) . R . set.seed(1) x1 = runif(100) x2 = 0.5*x1 + rnorm(100)/10 y = 2 + 2*x1+0.3*x2 + rnorm(100) . 마지막 줄이 두개의 설명변수를 이용한 중회귀모형이다. 회귀모형을 쓰시오. ($ beta$ 등을 이용하여) . Solution . (b) . 두 설명변수 $x_1$과 $x_2$ 사이에 상관관계(correlation)이 있는가? 산점도를 그려서 확인하여라 . Solution . (c) . 생성된 데이터를 이용하여 (a) 모형의 회귀계수를 추정하여라. 실제 회귀계수와 추정된 회귀계수와 비교하여라. $H_0 : beta_1 = 0$을 기각할 수 있는가? $H_1 : beta_2 = 0$을 기각할 수 있는가? . Solution . (d) . 이번에는 $x_1$만을 이용한 단순선형회귀 모형을 적합하여라. 결과를 분석하여라. $H_0 : beta_1 = 0$을 기각할 수 있는가 . Solution . (e) . 이번에는 $x_2$만을 이용한 단순선형회귀 모형을 적합하여라. 결과를 분석하여라. $H_0 : beta_2 = 0$을 기각할 수 있는가? . Solution . (f) . (c)-(e)의 결과가 서로 모순되는가? 설명하여라. . Solution . (g) . 새로운 데이터가 관측되었다고 하자.(이 데이터는 잘못 측정된 것이다.) . R . x1 &lt;- c(x1,0.1) x2 &lt;- c(x2,0.8) . 추가된 데이터를 이용하여 (c)-(e)를 다시 적합하여라. 결과가 어떻게 달라졌는가? 각 모형에서 새로 운 데이터는 이상점인가?(잔차가 기존에 있는 데이터에 비해 많이 큰가?) 아니면 영향점인가?(추가된 데이터로 인해 회귀계수의 값이 많이 바뀌었는가?) 설명하여라. . Solution . . Exercises for Logistic Regression . 1&#48264; . 두개의 설명변수 (X1 = 공부시간, X2 = 학부평점)를 이용하여 A학점을 받을 확률을 예측하기 위해 로지스틱 회귀모형을 적합하였다. 추정된 회귀계수는 $ beta_0 = −6, , beta_1 = 0.05, , beta_2 = 1$이다. . (a) . 40시간 공부하고, 평점이 3.5인 학생이 A를 받았을 확률을 예측하여라. . Solution . (b) . 평점이 3.5인 학생은 얼마나 공부를 해야 A를 받을 확률이 50%를 넘을 것인가? . Solution . 2&#48264; . 다음은 odds에 관한 문제이다. . (a) . 신용카드결재 문제에서 결재를 하지 못하는 경우(default)에 대한 odds가 0.37인 사람들이 실제로 defalut할 확률은 평균적으로 얼마인가? . Solution . (b) . 어떤 개인이 default할 확률이 16% 라고 하자. 그 사람이 default할 odds는 얼마인가? . Solution .",
            "url": "https://gangcheol.github.io/data-mining/2022/03/30/(4%EC%A3%BC%EC%B0%A8)-%EA%B3%BC%EC%A0%9C.html",
            "relUrl": "/2022/03/30/(4%EC%A3%BC%EC%B0%A8)-%EA%B3%BC%EC%A0%9C.html",
            "date": " • Mar 30, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "(1주차) 선형회귀분석",
            "content": "&#49440;&#54805;&#54924;&#44480;&#48516;&#49437; &#44592;&#52488; . 변수들간의 인과관계를 밝히고 모형을 적합하여 관심 있는 변수를 예측하거나 추론하기 위해 사용하는 분석기법 | . 선형회귀분석의 가정 오차의 등분산성 | 오차의 독립성 | 오차의 정규성 : Q-Q plot, Kolmogorov-Smirnov 검정, Shapiro-Wilk 검정을 확인하여 정규성을 확인한다. | . | . &#54924;&#44480;&#48516;&#49437; &#49884; &#44160;&#53664;&#49324;&#54637; . 0. 회귀모형이 통계적으로 유의한가 확인 . 1. 모형 내의 개별 회귀계수에 대한 검정 . 2. 모형에 설명력 $R^2$값을 통해 확인, 독립변수의 수가 많아지면 $adj-R^2$ 값을 확인 . 3. 잔차 plot을 통해 모형의 진단 . 4. 다중공선성의 확인 (10이상이면 다중공선성이 존재한다고 판단.) $ to$ car 패키지의 vif 함수 이용 . 5. 잔차분석 . R&#49892;&#49845; - &#45800;&#49692;&#49440;&#54805;&#54924;&#44480;&#48516;&#49437; . Cars93 데이터의 엔진크기(EngineSize)를 독립변수, 가격(Price)를 종속변수로 선정하여 단순 선형회귀분석을 실시한 후, 추정된 회귀모형에 대해 해석해보자. . library(MASS) library(lmtest) ## 더비왓슨 테스트를 위함 library(tidyverse) select &lt;- dplyr::select . . fit1 &lt;- lm(Price~EngineSize,data=Cars93) summary(fit1) . . Call: lm(formula = Price ~ EngineSize, data = Cars93) Residuals: Min 1Q Median 3Q Max -13.684 -4.627 -1.795 2.592 39.429 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 4.6692 2.2390 2.085 0.0398 * EngineSize 5.5629 0.7828 7.107 2.59e-10 *** Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 7.789 on 91 degrees of freedom Multiple R-squared: 0.3569, Adjusted R-squared: 0.3499 F-statistic: 50.51 on 1 and 91 DF, p-value: 2.588e-10 . plot(Cars93$EngineSize,Cars93$Price,lwd=2) abline(a=coefficients(fit1)[2],b=coefficients(fit1)[1],col=&quot;red&quot;,lwd=2) . . par(mfrow=c(1,2)) plot(fit1,1); plot(fit1,2) . . shapiro.test(resid(fit1)) . . Shapiro-Wilk normality test data: resid(fit1) W = 0.85365, p-value = 3.886e-08 . dwtest(fit1,alternative=&quot;two.sided&quot;) . . Durbin-Watson test data: fit1 DW = 1.1716, p-value = 2.236e-05 alternative hypothesis: true autocorrelation is not 0 . 1. 모형과 추정된 회귀계수는 모두 통계적으로 유의하다. . 2. 결정계수값과 수정된 결정계수 값이 각각 0.3569, 0.3499 로 산출되었다. . 3. F-통계량의 근거한 p-value값을 보아도 생성된 모델은 통계적으로 유의하다. . 4. 잔차 plot 을 그려본 결과 오차항의 정규성과 독립성 가정이 위배된 것 같다. . * 실제로 test 결과 위배되었다는 결론이 통계적으로 유의미했다. . 5. 따라서 모형의 식별 단계로 돌아가 새로운 모형을 적합할 필요가 있어보인다. . test &lt;- Cars93 %&gt;% select(EngineSize) %&gt;% sample_n(5) . . predict(fit1,test,interval=&quot;none&quot;) ##점추정 . . &lt;dl class=dl-inline&gt;123.0268912710567216.9076569678407324.1394793261868425.8083614088821523.5831852986217&lt;/dl&gt; predict(fit1,test,interval=&quot;confidence&quot;) # 회귀계수에 대한 신뢰구간을 고려한 구간 predict(fit1,test,interval=&quot;prediction&quot;) # 회귀계수에 대한 신뢰구간과 오차항을 고려한 구간 . . A matrix: 5 × 3 of type dbl fitlwrupr . 123.02689 | 21.14536 | 24.90842 | . 216.90766 | 15.14623 | 18.66909 | . 324.13948 | 22.07834 | 26.20061 | . 425.80836 | 23.42653 | 28.19019 | . 523.58319 | 21.61595 | 25.55043 | . A matrix: 5 × 3 of type dbl fitlwrupr . 123.02689 | 7.441846 | 38.61194 | . 216.90766 | 1.336654 | 32.47866 | . 324.13948 | 8.531732 | 39.74723 | . 425.80836 | 10.155035 | 41.46169 | . 523.58319 | 7.987560 | 39.17881 | . R&#49892;&#49845; - &#51473;&#54924;&#44480;&#48516;&#49437; . iris 데이터를 사용 | . R에 lm함수는 범주형 변수를 자동으로 더미변수로 변환해줌 | . fit2 &lt;- lm(Petal.Length~.,data=iris) summary(fit2) . . Call: lm(formula = Petal.Length ~ ., data = iris) Residuals: Min 1Q Median 3Q Max -0.78396 -0.15708 0.00193 0.14730 0.65418 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -1.11099 0.26987 -4.117 6.45e-05 *** Sepal.Length 0.60801 0.05024 12.101 &lt; 2e-16 *** Sepal.Width -0.18052 0.08036 -2.246 0.0262 * Petal.Width 0.60222 0.12144 4.959 1.97e-06 *** Speciesversicolor 1.46337 0.17345 8.437 3.14e-14 *** Speciesvirginica 1.97422 0.24480 8.065 2.60e-13 *** Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.2627 on 144 degrees of freedom Multiple R-squared: 0.9786, Adjusted R-squared: 0.9778 F-statistic: 1317 on 5 and 144 DF, p-value: &lt; 2.2e-16 . dwtest(fit2,alternative=&quot;two.sided&quot;) . . Durbin-Watson test data: fit2 DW = 1.6772, p-value = 0.03042 alternative hypothesis: true autocorrelation is not 0 . shapiro.test(resid(fit2)) . . Shapiro-Wilk normality test data: resid(fit2) W = 0.99389, p-value = 0.78 . library(car) vif(fit2) . . A matrix: 4 × 3 of type dbl GVIFDfGVIF^(1/(2*Df)) . Sepal.Length 3.736705 | 1 | 1.933056 | . Sepal.Width 2.648127 | 1 | 1.627307 | . Petal.Width18.496973 | 1 | 4.300811 | . Species28.551416 | 2 | 2.311569 | . &#52572;&#51201;&#54924;&#44480;&#48169;&#51221;&#49885;&#51032; &#49440;&#53469; . &#44032;. &#45800;&#44228;&#51201; &#48320;&#49688;&#49440;&#53469;(Stepwise Variable Selection) . 1. 전진 선택법 (forward selection) : 절편만 있는 상수모형에서 시작하여 중요하다고 생각되는 설명변수부터 차례로 추가한다. . 2. 후진 제거법 (backward elimination) : 모든 독립변수를 포함한 모형에서 출발하여 종속변수에 가장 적은 영향을 주는 변수부터 하나씩 제거하면서 더 이상 제거할 변수가 없을 때의 모형을 선택한다. . 3. 단계적 방법 (stepwise method) : 전진선택법에 의해 변수를 추가하면서 새롭게 추가된 변수에 의해 기존 변수의 중요도가 약화되면 해당변수를 제거한다. . &#45208;. &#48268;&#51216;&#54868;&#46108; &#49440;&#53469;&#44592;&#51456; . 모형의 복잡도에 따라 벌점을 주는 방식으로 $AIC, BIC$ 값이 주로 사용된다. | . R&#49892;&#49845; : &#45796;&#51473;&#54924;&#44480;&#47784;&#54805; + &#48320;&#49688;&#49440;&#53469;&#48277; . fit3 &lt;- step(lm(Price~ EngineSize +Horsepower +RPM + Width + Length + Weight,Cars93),direction = &quot;both&quot;) summary(fit3) . . Start: AIC=322.11 Price ~ EngineSize + Horsepower + RPM + Width + Length + Weight Df Sum of Sq RSS AIC - EngineSize 1 1.69 2556.1 320.17 - RPM 1 19.71 2574.1 320.82 &lt;none&gt; 2554.4 322.11 - Length 1 119.55 2674.0 324.36 - Weight 1 209.73 2764.2 327.45 - Width 1 585.01 3139.4 339.29 - Horsepower 1 720.84 3275.3 343.22 Step: AIC=320.17 Price ~ Horsepower + RPM + Width + Length + Weight Df Sum of Sq RSS AIC - RPM 1 49.36 2605.5 319.95 &lt;none&gt; 2556.1 320.17 + EngineSize 1 1.69 2554.4 322.11 - Length 1 140.92 2697.0 323.16 - Weight 1 208.09 2764.2 325.45 - Width 1 593.56 3149.7 337.59 - Horsepower 1 1476.65 4032.8 360.57 Step: AIC=319.95 Price ~ Horsepower + Width + Length + Weight Df Sum of Sq RSS AIC &lt;none&gt; 2605.5 319.95 + RPM 1 49.36 2556.1 320.17 + EngineSize 1 31.34 2574.1 320.82 - Length 1 132.02 2737.5 322.54 - Weight 1 279.31 2884.8 327.42 - Width 1 562.10 3167.6 336.12 - Horsepower 1 1898.74 4504.2 368.86 . Call: lm(formula = Price ~ Horsepower + Width + Length + Weight, data = Cars93) Residuals: Min 1Q Median 3Q Max -14.956 -2.578 -0.182 2.114 28.448 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 53.005861 16.532269 3.206 0.00188 ** Horsepower 0.129653 0.016190 8.008 4.46e-12 *** Width -1.480623 0.339813 -4.357 3.56e-05 *** Length 0.152968 0.072440 2.112 0.03755 * Weight 0.007339 0.002389 3.071 0.00283 ** Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 5.441 on 88 degrees of freedom Multiple R-squared: 0.6965, Adjusted R-squared: 0.6827 F-statistic: 50.48 on 4 and 88 DF, p-value: &lt; 2.2e-16 .",
            "url": "https://gangcheol.github.io/data-mining/r/jupyter/2022/03/03/(1%EC%A3%BC%EC%B0%A8)-%EC%84%A0%ED%98%95%ED%9A%8C%EA%B7%80%EB%B6%84%EC%84%9D.html",
            "relUrl": "/r/jupyter/2022/03/03/(1%EC%A3%BC%EC%B0%A8)-%EC%84%A0%ED%98%95%ED%9A%8C%EA%B7%80%EB%B6%84%EC%84%9D.html",
            "date": " • Mar 3, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://gangcheol.github.io/data-mining/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Github . github . Soundcloud . C.I.C . NLP . NLP . Data Mining . Data Mining . Bigdata Analysis . Bigdata Analysis .",
          "url": "https://gangcheol.github.io/data-mining/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://gangcheol.github.io/data-mining/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}